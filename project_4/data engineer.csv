address,company,employment_type,industry,job,requirements,responsibilities,salary,salary_type,seniority,title
"MARINA BAY FINANCIAL CENTRE, 8 MARINA BOULEVARD 018981",GOOGLE ASIA PACIFIC PTE. LTD.,Full Time,Engineering,"Strategic Cloud Data Engineer, Google Professional Services - Singapore","RequirementsMinimum qualifications: - Bachelor's degree in Computer Science, Mathematics or related technical field, or equivalent practical experience. - 2 years relevant work experience managing internal or client-facing projects to completion, troubleshooting clients' technical challenges and experience working with Engineering teams, Sales, Services, and customers. - Experience with data processing software (such as Hadoop, Spark, Pig, Hive) along with data processing algorithms (MapReduce, Flume). - Experience in writing software in one or more languages such as; Java, C++, Python, Go and/or JavaScript. Preferred qualifications: - Experience in working with/on data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools, environments, and data structures. - Experience in Big Data, information retrieval, data mining or Machine Learning as well as experiences in building multi-tier high availability applications with modern web technologies such as; NoSQL, MongoDB, SparkML, and Tensorflow. - Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments such as AWS, Azure, and Google Cloud Platform - Relevant experience in technical consulting.","Roles & ResponsibilitiesCompany overview: Google is not a conventional company, and we don’t intend to become one. True, we share attributes with the world’s most successful organizations – a focus on innovation and smart business practices comes to mind – but even as we continue to grow, we’re committed to retaining a small-company feel. At Google, we know that every employee has something important to say, and that every employee is integral to our success. We provide individually-tailored compensation packages that can be comprised of competitive salary, bonus, and equity components, along with the opportunity to earn further financial bonuses and rewards. Googlers thrive in small, focused teams and high-energy environments, believe in the ability of technology to change the world, and are as passionate about their lives as they are about their work. For more information, visit www.google.com/careers. The area: Google Cloud Google Cloud helps millions of employees and organizations empower their employees, serve their customers, and build what’s next for their business — all with technology built in the cloud. Our products are engineered for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. And our teams are dedicated to helping our customers — developers, small and large businesses, educational institutions and government agencies — see the benefits of our technology come to life. The role: Strategic Cloud Data Engineer, Google Professional Services - Singapore The Google Cloud Platform team helps customers transform and evolve their business through the use of Google’s global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you will help shape the future of businesses of all sizes use technology to connect with customers, employees and partners. Additional Role Description: As a Cloud Data Engineer, you'll guide customers on how to ingest, store, process, analyze and explore/visualize data on the Google Cloud Platform. You will work on data migrations and transformational projects and work with customers to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential platform challenges. You will travel to customer sites to deploy solutions and deliver workshops to educate and empower customers. Additionally, you'll work closely with Product Management and Product Engineering teams to build and constantly drive excellence in our products. Responsibilities: - Be a trusted technical advisor to customers and solve complex cloud infrastructure and networking challenges. - Create and deliver best practices recommendations, tutorials, blog articles, sample code and technical presentations adapting to different levels of key business and technical stakeholders. - Travel regularly, up to 30% of the time in-region for meetings, technical reviews, and onsite delivery activities.","$6,750to$13,500",Monthly,Executive,data engineer
"CHINA SQUARE CENTRAL, 18 CROSS STREET 048423",THOUGHTWORKS PTE. LTD.,Full Time,"Consulting , Information Technology",Senior Data Engineer,"RequirementsIdeally, you should have -   5+ years of experience building and deploying large scale data processing pipelines in a production environment Production-level hands-on experience working on HDFS, Java MapReduce, Hive, Apache Spark, Oozie etc. Solid understanding of YARN, Mesos, MPP Databases, SQL-on-Hadoop solutions like Impala etc. Experience working with, or an interest in Agile Methodologies, such as Extreme Programming (XP) and Scrum Knowledge of software best practices, like Test-Driven Development (TDD) and Continuous Integration (CI) Strong communication and client-facing skills with the ability to work in a consulting environment is essential Senior developers (7+ years) are expected to be the Architect for small and large enterprise projects. On larger projects, you are expected to work closely with the fellow architects to come up with the architecture and take it further. Desire to contribute to the wider technical community through collaboration, coaching, and mentoring of other technologists  If you relish the idea of being part of a community that extends beyond the work we do for our customers, you may find ThoughtWorks is the right place for you. If you share our passion for technology and want to help change the world with software, we want to hear from you! To apply, please submit your CV and tell us why you want to join ThoughtWorks. We will ask you to write code as part of your interview process, so be prepared! Our recruiters will be in touch.","Roles & ResponsibilitiesSingapore, SingaporeThoughtWorks Singapore is looking for talented engineers passionate about building large scale data processing systems to help manage the ever-growing information needs of our clients.    You will be responsible for -   Creating complex data processing pipelines, as part of diverse, high energy teams Designing scalable implementations of the models  Hands-on programming based on TDD, usually in a pair programming environment Deploying data pipelines in production based on Continuous Delivery practices Advising clients on the usage of different distributed storage and computing technologies from the plethora of options available in the ecosystem ","$5,700to$11,200",Monthly,Professional,data engineer
"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",NIBAARA TECHNOLOGIES PTE. LTD.,Full Time,Information Technology,Sr. Software engineer Data cap /  ICM,"Requirements Hands on experience in designing and developing applications using Datacap 9.1. 5-8 years of development experience using Java/J2EE related technologies Should possess good communication and problem solving skills Should have Aptitude and Attitude of learning and mastering new technologies and tools Working in client facing environment is a plus Working in Agile Methodology is a plus Working experience in Banking Domain is an added advantage Profound insight of dojo, navigator, VB script and Java script. Working experience with Relational Databases, PL/SQL. Experience in Oracle preferred. Experience in Datacap solution deployment. Object oriented analysis and design using common design patterns. Good to have experience in installation of Datacap. Good to have experience in designing and developing applications using Datacap 8.1, ASP.net and VB script. Good to have knowledge of OCR (optical character recognition) ","Roles & Responsibilities Ensure the timely completion of the tasks assigned Ensure to follow the Technology and Process Standards, set for the project Produce high quality technical documentation Develop Low Level Design Undertake development tasks with minimal supervision, including programming and testing To monitor progress and provide timely updates to Lead On need basis should able to work as back up lead Technical Design documents Developed Programs with high quality and as per the planned schedule Developed Programs with high quality, with process standards and as per the planned schedule Unit Test Cases Interacts with Technical Lead, BAs and testers for the project during the project lifecycle Interacts with the Client managers, for status review meetings ","$5,000to$7,000",Monthly,Professional,data engineer
"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",NIBAARA TECHNOLOGIES PTE. LTD.,Full Time,Information Technology,Data cap /  ICM Engineer,"Requirements Atleast 2-4 years of development experience using Datacap 9.1, Java/J2EE related technologies Should possess good communication and problem solving skills Should have Aptitude and Attitude of learning and mastering new technologies and tools Working in client facing environment is a plus Working in Agile Methodology is a plus Working experience in Banking Domain is an added advantage ","Roles & Responsibilities Hands on experience in designing and developing applications using Datacap 9.1. Profound insight of dojo, navigator, VB script and Java script. Working experience with Relational Databases, PL/SQL. Experience in Oracle preferred. Experience in Datacap solution deployment. Object oriented analysis and design using common design patterns. Good to have experience in designing and developing applications using Datacap 8.1, ASP.net and VB script. Good to have knowledge of OCR (optical character recognition) Technical Design documents Developed Programs with high quality and as per the planned schedule Developed Programs with high quality, with process standards and as per the planned schedule Unit Test Cases   ","$4,500to$6,000",Monthly,Professional,data engineer
,Company Undisclosed,Permanent,Banking and Finance,Data Engineer,"RequirementsTechnical and Behavioral Competencies / Specific Qualifications (if required)  Technical Competencies   Implementation experience with of Big Data technologies including large data stores and Machine Learning Knowledge of various Statistical Analysis, Probabilistic Analysis and Data Visualization techniques preferred Advanced knowledge of SQL and/or NoSQL systems Basic understanding of project lifecycle stages    Behavioral Competencies  Ability to manage conflicting requests on time in a continually fast moving environment and operating in a global team Must be a self-starter with attention to detail and strong communications (written and oral) skills    Preferred qualifications:   Experience with Front Office traders, IT support and Operation teams in a Capital Markets domain. Knowledge of JAVA, J2EE, JavaScript, JSON, Perl, SQL, and PL/SQL would be a plus Experience with machine learning, natural language processing, AI, or robotics is a plus    Qualifications and Experience  Master’s degree in engineering, math, statistics, physics or other technical field   ","Roles & ResponsibilitiesPosition Purpose The APAC GM & ALMT IT TP Engineering group runs Trade Processing and Regulatory Projects for Global Market and ALMT business and operations.   Responsibilities  Partner with senior business sponsors, platform supervision, compliance, and business management to enhance and augment the holistic supervisory and surveillance platform Investigate and analyze data on the surveillance platform as key input to enhance existing data strategy on multi-vector analysis using various natural language processing and machine learning techniques Primary focus is to investigate and analyze available structured and un-structured information, apply data mining techniques, perform statistical analysis, and provide methods (algorithmic and technical) to create actionable alerts and reduce false-positives Develop and enhance software solutions to meet business requirements Gather and document technical requirements and specifications Work with the project PM and BA’s on project planning and deliverable Work on multiple tasks and respect aggressive schedule Work in a fast paced environment. ","$6,500to$13,000",Monthly,"Executive, Senior Executive",data engineer
,DBS BANK LTD.,"Permanent, Full Time",Banking and Finance,"VP / AVP, Senior Data Engineer, Group Consumer Banking and Big Data Analytics Technology (180003L2)","Requirements Experience in big data and machine learning The ability to work with loosely defined requirements and exercise your analytical skills to clarify questions, share your approach and build/test elegant solutions in weekly sprint/release cycles. Development experience in Java/Scala and pride in producing clean, maintainable code Practical experience in clustering high dimensionality data using a variety of approaches Real world experience in solving business problems by deploying one or more machine learning techniques Experience creating pipelines to analyze data, extracted features and updated models in production. Independence and self-reliance while being a pro-active team player with excellent communication skills. Hands-on development with key technologies including Scala, Spark, and other relevant distributed computing languages, frameworks, and libraries.  Experience with distributed databases, such as Cassandra, and the key issues affecting their performance and reliability.  Experience using high-throughput, distributed message queueing systems such as Kafka. Familiarity with operational technologies, including Docker (required), Chef, Puppet, ZooKeeper, Terraform, and Ansible (preferred).  An ability to periodically deploy systems to on-prem environments.  Mastery of key development tools such as GIT, and familiarity with collaboration tools such as Jira and Confluence or similar tools.  Experience with Teradata SQL, Exadata SQL, T-SQL Strong experience in graph and stream processing Experience in migrating SQL from traditional RDBMS to Spark and BigData technologies Experience in building language parsers using ANTLR, query optimizers and automatic code generation In-depth knowledge of database internals and Spark SQL Catalyst engine ","Roles & Responsibilities Design and implement key components for highly scalable, distributed data collection and analysis system built for handling petabytes of data in the cloud.  Work with architects from other divisions contributing to this analytics system and mentor team members on best practices in backend infrastructure and distributed computing topics.  Analyze source data and data flows, working with structured and unstructured data. Manipulate high-volume, high-dimensionality data from varying sources to highlight patterns, anomalies, relationships and trends Analyze and visualize diverse sources of data, interpret results in the business context and report results clearly and concisely. Apply data mining, NLP, and machine learning (both supervised and unsupervised) to improve relevance and personalization algorithms. Work side-by-side with product managers, software engineers, and designers in designing experiments and minimum viable products. Build and optimize classifiers using machine learning techniques and enhance data collection procedures that is relevant for building analytic systems. Discover data sources, get access to them, import them, clean them up, and make them “model-ready”. You need to be willing and able to do your own ETL. Create and refine features from the underlying data. You’ll enjoy developing just enough subject matter expertise to have an intuition about what features might make your model perform better, and then you’ll lather, rinse and repeat. Run regular A/B tests, gather data, perform statistical analysis, draw conclusions on the impact of your optimizations and communicate results to peers and leaders. ","$7,000to$14,000",Monthly,"Middle Management, Manager",data engineer
,HYDROINFORMATICS INSTITUTE PTE. LTD.,Full Time,Professional Services,Data Engineer,"Requirements Masters or PhD in one of the mathematical sciences, or engineering Programming capability the data science languages – Python, R, and preferably in core languages – C, C++ Understanding and ability to code algorithms related to multivariate statistics, spatio-temporal statistics and time series analysis, and common machine learning algorithms in clustering, dimensionality reduction, neural networks etc. Familiarity with open source data visualization frameworks with D3.js is preferred Understanding of database concepts and distributed processing systems and knowledge of one of the frameworks – Hadoop, Spark, NOSQL etc. Excellent written and oral presentation skills Interest in developing environmental solutions ","Roles & ResponsibilitiesRoles and Responsibilities:  Implement data wrangling, preprocessing and preparation scripts for available raw environmental data Implement statistical and machine learning algorithms for data analysis and knowledge derivation Implement data visualization solutions for post processing and communication to internal and external teams Create and maintain databases for different kinds of geo-spatiotemporal data Monitor and manage data pipelines and test production codes ",,,Executive,data engineer
,Company Undisclosed,Full Time,Information Technology,Cyber Security Big Data Engineer,"Requirements Knowledge of Cybersecurity organization practices, operations, risk management processes, principles, architectural requirements, engineering and threats and vulnerabilities, including incident response methodologies Ability to collaborate with high-performing teams and individuals throughout taghe firm to accomplish common goals Proficiency in the use of skills tools, staying current with skills, participating in multiple forums Experience with Agile and can work with at least one of the common frameworks is highly desired Ability to analyze vulnerabilities, threats, designs, procedures and architectural design, producing reports and sharing intelligence Strong research, analytical and problem solving skills Independent problem-solving, highly motivated and self-directing Ability to write and debug administrative and reporting tools in some programming languages (Shell/Perl or Python, Scala/Java/R, C/C++, HTML5, or other experiences acceptable) Comfortable with most aspect of operating system administration such as tweaking, hardening and configuring services A solid understanding of Unix-based operating systems, including paging/swapping, IPC, drivers and filesystem (inode, partitions, etc.) Experience with host and network security (identity/password management, ACLs, file permissions and integrity) Strong interpersonal and communication skills; capable of writing documentation, training users in complex topics, making presentations to junior and very senior audience Ability to work under pressure in a fast-paced environment while remaining productive and professional; exercise patience and ability to multi task    Bonus Points  Experience with hadoop ecosystem: Hadoop, Spark, Map/Reduce, Hive/Pig, Impala/Drill, etc. Experience with Data Science: MLlib, Scikit, h2o, TensorFlow, Pytorch, Caffe, Singa, etc. Experience with NoSQL stacks: Elasticsearch, MongoDB, etc. Experience with SIEM products: Qradar, Arcsight, Splunk, etc. Experience with messaging and data transport tools: Kafka, NiFi, LogStash, Syslog-ng, rsyslog, etc. Experience with Link Analysis tools and GraphDBs Experience with data visualization tools: Hue, Kibana, Qlikview, Tableau, etc. Knowledge in RIA: HTML5, node.js, bootstrap, angular, extJS, etc. ","Roles & ResponsibilitiesWorking in Cybersecurity takes pure passion for technology, speed, a constant desire to learn, and above all, vigilance in keeping every last asset safe and sound. You’ll be on the front lines of innovation, working with a highly-motivated team laser-focused on analyzing, designing, developing and delivering solutions built to stop adversaries and strengthen our operations. Your research and work will ensure stability, capacity and resiliency of our products in emerging industry trends. Working in tandem with your internal team, as well as technologists and innovators across our global network, your ability to identify threats, provide intelligent analysis and positive actions will stop adversaries and strengthen our products.   Responsibilities  Focus on the development of tools and technologies that are at the core of the company’s capabilities to manage, monitor and hunt for cyber security incidents Architecture and development of large scale solution (big data) to be used in a very large production environment System, network and application troubleshooting Provide engineering support for cyber security products developed ","$7,000to$12,000",Monthly,Professional,data engineer
,DBS BANK LTD.,"Permanent, Full Time",Banking and Finance,"VP / AVP, Machine Learning Engineer, Group Consumer Banking and Big Data Analytics Tech (180003YE)","Requirements PhD/Masters/Bachelors in Computer Science, Computer Engineering, Statistics, Applied Mathematics, or related disciplines.  Excellent understanding of software engineering principles and design patterns. Excellent programming skills in either Python, Scala, or Java. In-depth understanding of data science and machine learning technologies and methodologies. Good working knowledge of high performance computing, parallel data processing, and big data stack, e.g. Spark and Hadoop/Yarn. Experience to one or more commercial / open source data warehouses or data analytics systems, e.g. Teradata, is a big plus. Experience to one or more NoSQL databases is a big plus. Hands-on experience in Cloud platforms, e.g. AWS, or containerization/ virtualization platforms, e.g. Docker/Kubernetes, is a big plus. Experience to any data science or machine learning platform, e.g. IBM Data Science Experience or Cloudera Data Science Workbench, is a big plus. Exposure to mainframe system is a plus. Passion about machine learning and data-driven intelligence system. Excellent communication and presentation skills in English. Team player, self-starter, ability to work on multiple projects in parallel is necessary. 2+ years of experience in machine learning system or data science research 5+ years of experience in software engineering or DevOps automation or data engineering Experience working in multi-cultural environments ","Roles & ResponsibilitiesJob Purpose    Build and improve machine learning and analytics platform. Work with data scientists to create, optimize and productionize of machine learning models for various business units within the organization. Keep innovating and optimizing data and machine learning workflow to enable data-driven business activities at large scale.    Responsibilities   Build and improve machine learning and analytics platform.       Apply cutting edge technologies and tool chain in big data and machine learning to build machine learning and analytics platform. Keep innovating and optimizing the machine learning workflow, from data exploration, model experimentation/prototyping to production. Provide engineering solution and framework to support machine learning and data-driven business activities at large scale. Perform R&D on new technologies and solutions to improve accessibility, scalability, efficiency and us abilities of machine learning and analytics platform.     Work with data scientists to build end-to-end machine learning and analytics solution to solve business challenges.       Turn advanced machine learning models created by data scientists into end-to-end production grade system. Build analytics platform components to support data collection, exploratory, and integration from various sources being data API, RDBMS, or big data platform. Optimize efficiency of machine learning algorithm by applying state-of-the-art technologies, i.e. distributed computing, concurrent programming, or GPU parallel computing.      Establish, apply and maintain best practices and principles of machine learning engineering.       Study and evaluate the state of the art technologies, tools, and frameworks of machine learning engineering. Contribute in creation of blueprint and reference architecture for various machine learning use cases. Support the organization in transformation towards a data driven business culture.     Work Relationships  Internal        Work closely with data scientists, business team, and project managers to provide machine learning and data-driven business solution.  Collaborate with other technology teams to build platform and framework to enable machine learning and data analytics activities at large scale     External        Maintain engineering principles and best practices of machine learning framework and technologies.   ","$7,000to$14,000",Monthly,"Middle Management, Manager",data engineer
"SHAW TOWERS, 100 BEACH ROAD 189702",PEOPLE PROFILERS PTE. LTD.,Permanent,Information Technology,Lead Data Engineer (Established FinTech Co / East),"RequirementsRequirements:  Preferably at least a Bachelor’s in a quantitative discipline (e.g. Computer Science, Engineering, or Mathematics) 8 years related work experience, in building high scalability, low latency systems Good knowledge & hands-on skills in at least 2 of these: Python/Java/Scala Knowledge of machine learning techniques (e.g. Bayesian methods, regression techniques) Good knowledge of algorithm generation & data structures. Knowledge and interest in data mining, machine learning, natural language processing, or information retrieval Prior exposure to NoSQL databases (development). Familiar with SQL and database technologies. Preferably have knowledge/experience in cloud computing technologies (e.g. AWS/Azure) Familiar with Linux environment. Knowledge in use of Hadoop/Spark  All Successful candidates can expect a very competitive remuneration package and a comprehensive range of benefits. Interested applicants may wish to email your resume in a detailed Word format to ruth.gan@peopleprofilers.com. Please include last drawn and expected salaries and notice period. We regret that only shortlisted candidates will be notified.   Gan Huiru  Recruitment Consultant Tel: +65 6594 9897 Fax +65 6835 7890 Address: 100 Beach Road #33-06 Shaw Tower Singapore 189702 Email: ruth.gan@peopleprofilers.com EA License Number: 02C4944 Registration Number: R1768917","Roles & Responsibilities Join a rapidly expanding and reputable company in FinTech industry  Attractive remuneration package Great benefits (work-life balance, fun work environment, health insurance, dental plan) Junior & senior positions available    Position Overview:  Support our team in developing cutting-edge solutions for the FinTech industry, keeping security standards in mind.    Responsibilities:  Building robust batch and streaming data pipeline for production-grade data products/platforms (tools: Hadoop, Spark) Creating web services or APIs to connect analytical stacks to application layers Building and maintaining both cloud and on-premise data infrastructure Data cleaning, & pre-processing (e.g. with images/text) Analyse requirements and deliver suitable solutions Write code according to best practices, and which meets security standards Keep up to date with new technologies ","$7,500to$10,000",Monthly,Senior Executive,data engineer
"TRIVEX, 8 BURN ROAD 369977",SANDBOX CONSULTING PTE. LTD.,"Permanent, Contract, Full Time",Information Technology,Data Engineer,"Requirements Experience and passion for data engineering in a big data environment using Cloud platforms - AWS, GCP or Azure  Experience with building production-grade data pipelines, ETL/ELT data integration  Interested in being the bridge between engineering and analytics  Knowledgeable about system design, data structure and algorithms  Familiar with data modelling, data access, and data storage infrastructure like Data Mart, Data Lake, and Data Warehouse.","Roles & Responsibilities Design, build, launch and maintain efficient and reliable large-scale batch and real-time data pipelines with data processing frameworks  Integrate and collate data silos in a manner which is both scalable and compliant  Collaborate with Project Manager, Frontend Developers, UX Designers and Data Analyst to build scalable data-driven products  Responsible for developing backend APIs & working on databases to support the applications  Working in an Agile Environment that practices Continuous Integration and Delivery  Working closely with fellow developers through pair programming and code review process","$5,500to$6,000",Monthly,Professional,data engineer
,Company Undisclosed,"Contract, Full Time",Information Technology,Data Engineer,"Requirements Experience and passion for data engineering in a big data environment using Cloud platforms - AWS, GCP or Azure Experience with building production-grade data pipelines, ETL/ELT data integration Interested in being the bridge between engineering and analytics Knowledgeable about system design, data structure and algorithms Familiar with data modelling, data access, and data storage infrastructure like Data Mart, Data Lake, and Data Warehouse.   ","Roles & Responsibilities Design, build, launch and maintain efficient and reliable large-scale batch and real-time data pipelines with data processing frameworks Integrate and collate data silos in a manner which is both scalable and compliant Collaborate with Project Manager, Frontend Developers, UX Designers and Data Analyst to build scalable data-driven products Responsible for developing backend APIs & working on databases to support the applications Working in an Agile Environment that practices Continuous Integration and Delivery Working closely with fellow developers through pair programming and code review process ","$5,000to$7,000",Monthly,Executive,data engineer
"UPS HOUSE, 22 CHANGI SOUTH AVENUE 2 486064",UPS ASIA GROUP PTE. LTD.,Permanent,Logistics / Supply Chain,APAC Data Engineer,"RequirementsSkills and Qualifications  Possess a Bachelor’s Degree or Master’s Degree in Computer Science, Information Systems or related discipline. Minimum 2 years of relevant experience in similar capacity using software/tools for big data, SQL and NoSQL databases and object oriented/object function scripting languages. Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with structured and unstructured datasets. A successful history of manipulating, processing and extracting value from large disconnected datasets. Possess solid project management and organizational skills. Prior experience in supporting and working with cross-functional teams in a dynamic environment. ","Roles & ResponsibilitiesSummary The Data Engineer will be responsible for expanding and optimizing the data and data pipeline architecture, data flow and collection for the Data Science team and creating API’s to integrate the models with production systems. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support the data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities:  Create and maintain optimal data pipeline architecture. Assemble large, complex data sets from multiple data sources that meet functional / nonfunctional business requirements. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and no SQL technologies. Develop data features that will serve as inputs to AI/Machine Learning/OR techniques. Build analytics tools that utilize the data pipeline to provide actionable insights into key business performance metrics. Develop data design based on exploratory data analysis to meet stated business need. Develop procedures to monitor model and production system performance/integrity. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Review and create repeatable solutions through written project documentation, process flowcharts, logs, and commented clean code to produce datasets that can be used in analytics and/or predictive modeling. Act as subject matter expert with investigating and evaluating emerging technologies. Articulate potential competitive market benefits of new technologies to senior management. Maintain broad understanding of implementation, integration, and interconnectivity issues with emerging technologies. ","$5,256to$7,008",Monthly,Senior Executive,data engineer
,DBS BANK LTD.,"Permanent, Full Time",Banking and Finance,"AVP  /  Senior Associate, Data Engineer, IBG Digital, Institutional Banking Group (1800044U)","Requirements Master’s Degree in software Engineering, Computer Science or related fields with minimum 3 years data engineering work experience in big data analytics environment Strong in data engineering skills with big data stack (Hadoop, Spark, Kafka, etc) Strong in transactional SQL, Enterprise Data Warehouse Experience with Graph Database, NoSQL databases Experience with Feature Engineering Experience with Master Data Management Experience with scripting languages: UNIX/Linux Shell, SQL, Python (Pandas, PySpark etc), Scala, R, etc ","Roles & ResponsibilitiesJob Purpose  The Data Engineer will provide big data engineering support to the Institutional Banking Group (IBG) Business Analytics Team in various data science projects. This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be one excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, Perform ETL/ELT, Data Modelling, Data Profiling, Data Cleansing, Feature Engineering tasks as part of Data Analytics Life Cycle (DALC); Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management ","$5,500to$11,000",Monthly,"Manager, Senior Executive",data engineer
,DBS BANK LTD.,"Permanent, Full Time",Banking and Finance,"AVP  /  Senior Associate, Data Engineer, Analytic Center of Excellence, Transformation Grp (180003G6)","Requirements Degree in Computer Science with minimum 3 years data engineering work experience in big data analytics environment Excellent data engineering skills with open source big data stack Experience building and optimizing ‘big data’ data pipelines, architectures and data sets Strong analytic skills related to working with unstructured datasets Experience with big data tools: Hadoop, Spark, Kafka, etc Experience with relational SQL and NoSQL databases Experience with object-oriented/object function scripting languages: Python, Scala, etc Familiar with deployment and optimization of open source big data analytic stack on distributed environment Familiar with compiling, deploying and configuring open source data science tools including Python, R, Spark, etc Familiar with deploying analytic projects and data science products to production Excellent programming skills ","Roles & ResponsibilitiesJob Purpose The Data Engineer will provide big data engineering support to the Analytic Center of Excellence.  This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.   Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies; Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management ","$5,500to$11,000",Monthly,"Manager, Senior Executive",data engineer
71 AYER RAJAH CRESCENT 139951,WORKATO PTE. LTD.,"Permanent, Full Time","Engineering, Information Technology",Data Engineer,"Requirements  Bachelor's degree in Computer Science, Computer Engineering, Business Administration, Mathematics or a related field   3+ years of industry experience as a Data Engineer or related specialty (e.g., Business Intelligence Engineer, Data Scientist)   Experience in data modeling, ETL development, and Data warehousing.   Data Warehousing Experience with Oracle, Redshift, Teradata, etc.   Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space   Experience in continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers   Experience building data products incrementally and integrating and managing datasets from multiple sources   Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets     Bonus Points   Experience leveraging Python, R or Matlab to manipulate data and set up automated processes as per business requirements   Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)   Experience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies   Strong ability to interact, communicate, present and influence within multiple levels of the organization   Track record of manipulating, processing, and extracting value from large datasets   Excellent communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions   Master's degree  ","Roles & ResponsibilitiesRole We are seeking a talented, self-directed Data Engineer to design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for our business stakeholders. Implement data structures using best practices in data modeling and ETL/ELT processes. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture. Analyze source data systems and drive best practices in source teams. Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance. Produce comprehensive, usable dataset documentation and metadata. Evaluate and make decisions around dataset implementations designed and proposed by peer data engineers. Evaluate and make decisions around the use of new or existing software products and tools. Mentor junior data engineers.  The ideal candidate relishes working with data, enjoys the challenge of highly complex technical contexts, and, above all else, is passionate about data and analytics. He/she is an expert with data modeling, ETL design and business intelligence tools and passionately partners with the business to identify strategic opportunities where improvements in data infrastructure creates outsized business impact. He/she is a self-starter, comfortable with ambiguity, able to think big (while paying careful attention to detail) and enjoys working in a fast-paced team. The ideal candidate needs to possess exceptional technical expertise in large scale data warehouse and BI systems with hands-on knowledge on SQL, Distributed/MPP data storage, and AWS services (S3, Redshift, EMR, RDS).   Responsibilities   Design, implement, and support a platform providing ad hoc access to large datasets   Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL   Implement data structures using best practices in data modeling, ETL/ELT processes, and SQL, and Redshift   Build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark   Build and deliver high quality datasets to support business analysis and customer reporting needs   Interface with business customers, gathering requirements and delivering complete data structures  ","$2,000to$10,000",Monthly,Executive,data engineer
"PAYA LEBAR SQUARE, 60 PAYA LEBAR ROAD 409051",OBSERVATIONAL AND PRAGMATIC RESEARCH INSTITUTE PTE. LTD.,"Permanent, Full Time","Engineering, Information Technology",Data Engineer,"RequirementsQualifications  Bachelor degree in Computer Science, Engineering, Maths or equivalent qualification  Required Experience  Strong working knowledge of SQL (Essential) Experience working with large databases  Preferred Experience  Experience of developing and maintaining data dictionaries for databases Knowledge of statistical analysis tools (e.g. R, STATA, SPSS, SAS) Interest and knowledge of epidemiology, public health and clinical research ","Roles & ResponsibilitiesThe Company OPRI is an academic research institution striving to improve the lives of patients through global research. OPRI has been leading the paradigm shift in real world evidence for the past 12 years, by delivering pragmatic clinical trials, disease registries and database research. The Role We are looking for a Data Engineer to work alongside our research, statistical and database teams in the UK, Singapore and Australia (Brisbane). In this position you will gain invaluable experience within an internationally recognised research organisation involved in analysis and dissemination of data from large-scale observational studies and pragmatic randomised controlled trials. The successful candidate will have high attention to detail, strong time management skills, and most importantly experience in the management and engineering of relational databases. Your responsibilities  Design, construct, install, test and maintain data collection and management systems:     Integrate data management technologies and software engineering tools for custom data collection applications Programming knowledge: Employ a variety of languages and tools (e.g. scripting languages) to combine systems together Ensure seamless integration of data across multiple databases       SQL, queries   Building APIs for data consumption Integrating external or new datasets into existing data pipelines Continuously monitoring and testing the system to ensure optimized performance   Build and maintain data collection platforms for specific organisational projects     Set up automated integration processes for Patient Reported Outcomes into various data collection platforms   EMR/EDC integration with Registry Database     Data collected via Registry EDCs to be uploaded into EMRs Data collected via site specific EMRs/EDCs to be uploaded into Registry EDCs    The role is for a permanent full-time position. Salary is dependent on qualifications and experience. Immediate start is available.","$3,000to$6,000",Monthly,Junior Executive,data engineer
"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",SCHELLDEN GLOBAL PTE. LTD.,Full Time,Information Technology,Big Data Engineer,"RequirementsCommercial software engineering: You have 3+ years of professional software development experience with languages and systems such as Java, Python (PySpark), and version control (git), with good analytical & debugging skills.  Big data: You have extensive experience with data analytics and working knowledge of big data infrastructure such as Hadoop Eco System, HDFS, Spark, Google Cloud, Big Query, Data Flow (nice to have). You've routinely built data pipelines with gigabytes/terabytes of data and understand the challenges of manipulating such large datasets.  Data Modeling: Flair for data, schema, data model, PL/SQL, Star & snow flake schema, how to bring efficiency in data modeling for efficient querying data for analysis, understands criticality TDD and develops data validation techniques.  Real Time Systems: Understands evolution of databases for in-memory, NoSQL & indexing technologies along with experience on real-time & stream processing systems like kafka, Storm.  Project management: You demonstrate excellent project and time management skills, exposure to scrum or other agile practices in JIRA.","Roles & ResponsibilitiesMassive data: You will source / examine, analyze, engineer data pipelines for gigabytes/terabytes of structured and unstructured data with our platform to create value for customers.  Pushing the limits: This role will be on the cutting edge of our Data / Machine Learning platform. As we push to solve more of our customer challenges, you will be prototyping new features, tools and ideas. Innovate at a very fast pace to maintain our competitive edge.  Linux hacking: You will be masterfully using the command line, including tools like vi/emacs and understanding beyond basics of grep, bash, awk, sed, etc to aggressively dive into data, systems, and compute platforms to get the results you are seeking.  Production deployment: You will be responsible for integration and deployment of the machine learning pipelines into production where your ideas can come to life.  Coordinate and work with cross functional teams, sometimes located at different geo locations.","$4,000to$8,000",Monthly,Middle Management,data engineer
"THE RIVERWALK, 20 UPPER CIRCULAR ROAD 058416",PERX TECHNOLOGIES PTE. LTD.,Permanent,Information Technology,Data Engineer,"RequirementsMS in computer science or a related field OR BS in computer science and 3 years of experience in software engineering. Backend development experience with a solid foundation in data pipelines, distributed systems, large-scale data processing. Experience with DBs like AWS Redshift, PostGREs, MySQL. Experience with ETL and query language. Proficiency with Python, Scala or Java. Experience with Ruby is a plus. Experience with Linux/Unix systems and AWS / cloud environments. Working knowledge of MapReduce, Hadoop, HDFS. Experience with Spark is a big plus!  ","Roles & ResponsibilitiesWhat You’ll Do:   As a Data Engineer on the Analytics team, you will be the “source of truth” for Perx’s most fundamental data - such as end-customer engagement and client usage data - along with core metrics such as daily (DAU) and monthly active users (MAU). Alongside designing & implementing the plumbing & infrastructure that will power the Analytics frameworks, you will also help lead the company’s decision to use bleeding-edge data technologies and features, working directly with our infrastructure team to integrate them into the services you design at scale. In doing so, you will help empower the Engineering department, tens of co-workers, thousands of marketing analysts and millions of end customers to dream of new insights and new possibilities.   Who You Are: You are a go-getter & dreamer, wanting to join a community of extremely talented, forward-thinking & diverse engineers in the industry & region. You gain happiness in building & scaling resilient, robust, well performing, and end-to-end tested distributed systems that can power the most business-critical applications. You want to learn, work with, and leverage on cutting-edge open-source technologies. The ideal candidate has experience with and/or history of contributions to Python, Hadoop, Spark, Redshift, Cassandra, PostGREs, Ruby (on Rails) or similar technologies. You have experience in distributed systems, database internals, or performance analysis.","$5,000to$8,000",Monthly,Executive,data engineer
79 AYER RAJAH CRESCENT 139955,6ESTATES PTE. LTD.,Full Time,Information Technology,Big Data Engineer,"Requirements1. Minimum Bachelor’s degree in Computer Science and 3+ years of experience in administration/architecture in the field of big data specific to Hadoop, HBase 2. Excellent implementation skills in Java or Scala. 3. Expert knowledge of Linux/Unix programming. 4. Experience in Apache Hadoop, Spark, Zookeeper, Elasticsearch, etc. 5. Strong knowledge of distributed system architecture and implementation. 6. Knowledge of Information Retrieval / NLP / Data Mining is a plus. 7. Fluent in English and Chinese to liaise with Chinese speaking associates","Roles & ResponsibilitiesWe are looking for the right individual who has the passion and desire to crack the code for this very hot field in AI and Big Data. 6Estates engineers build tools and solutions that ensure the delivery of high quality software for our stakeholders. For someone who wants to learn and grow, this role provides you the unique opportunity to work along with all the experts of different fields. As a Big Data Engineer, you will work with a team of talents to design, architect, and develop on the big data analytics platform. You will also touch on researching, introducing modern technologies and integrating your amazing innovations and ideas into our production systems.","$4,000to$8,000",Monthly,Professional,data engineer
,DBS BANK LTD.,"Permanent, Full Time",Banking and Finance,"AVP / Senior Associate, Lead Development Engineer, Grp Consumer Banking & Big Data Analytics (180002YV","Requirements The candidate need to have minimum of 5 years IT experience with Avaloq Release Management and infrastructure project delivery. The candidate should have strong infrastructure technical background with hands on Open Systems platform such as Solaris, Linux, virtualization, network, and storage. Moderate information security knowledge Have a good understanding of ITIL processes and project management processes. Development experience in SQL & PL/SQL, preferably in Oracle 11g / 12c environment Must have hands-on experience in IBM MQ series, Connect Direct and SSH such as file transfer tools Good to have knowledge on the Micro services and cloud services A Bachelor’s degree in Computer Science (or equivalent experience) 5 -8 years of development and delivery experience Able to perform Unix / Linux scripting. Monitor and address issues relating to capacity constraints and performance related items. Analyze and perform database performance tuning. Develop and maintain high-performance, scalable utilities to support technology research and data transformation. Contribute to the establishment and maintenance of distributed computing platform / Messaging services Good leadership skills in working with Application teams and service providers in defining and executing infrastructure deployment plan, coordination of all infrastructure required activities, cutover/migration strategy and test plan. Good in documentation, tracking, form submission, raising change request and project status reports.  Should be an effective communicator with good people management skills to handle diverse groups/teams in the project. Should be a proactive self-starter with strong analytical skill, team-player, independent, pro-active, resourceful.  ","Roles & Responsibilities Manage the Avaloq Configuration / Release Management tasks and work on the BAU tasks in non-production environments Env Planning (DB, application), Avaloq ICE Streaming, Release calendars preparation and communicate with all stakeholders Definition of Connect Direct (NDM), IBM MQ, Avaloq Tools Upgrade, TWS Definition and Batch support, and FIX Platform interface setup Projects and Enhancement requests. Plan and manage the end-to-end deployment and delivery of IT infrastructure for Bank’s project from deployment planning, setup & testing, pre-production readiness to production cutover. Collaborate with Architecture and Engineering team, Application teams, Infrastructure teams, and service providers in delivering quality IT solutions and services to meet business objectives. Ensure the project meet schedule and within the allocated budget and resources. Adhere to the bank's Project Management, Deployment and Change management process.  Prepare and submit the necessary change requests and requisition forms for the deployment of infrastructure for the project, such as facility request, IP/DNS/Hostname request, SAN requisition, etc. Conduct proper transition from Project to Operations before project closure. Prepare all project documentation such as SOM and status reports, assure report accuracy and timeliness. e.g. Weekly Project Status report, etc. Work collaboratively with technology team and vendors, provide single point of contact and drive resolution of issues that arise in projects. Maintain business partnership with Line of Business (LOBs) and constantly collect and manage user’s demands and applications’ infra requirements. Coordinate with DBA, Solaris, Linux, Windows, Network, ID Mgmt and other Infra admin teams on the system issue related tasks Co-ordinate with various Infra teams to apply the OS / DB / MQ patches  Apply the innovative thinking to automate the manual tasks, provide infrastructure services effortlessly, improve performance and resilience of the systems. ","$5,500to$11,000",Monthly,"Manager, Senior Executive",data engineer
,DBS BANK LTD.,"Permanent, Full Time",Banking and Finance,"VP / AVP, Development Engineer,  Group Consumer Banking & Big Data Analytics Tech, T&O (180001ZC)","Requirements Solid experience in Java, JavaScript, IBM MQ & JMS, Spring boot, Hibernate, Eclipse, JUnit, Apache, etc. Hands-on Design, Development, Deployment, Configuration & Support of various Integration platform components 10 + years of experience in the Technology field including hands on in both development and support of SOA or Microservices Proven experience in design and development of APIs using API Gateways including Gateway deployment, configuration, policy development, migration, debugging and troubleshooting Working knowledge of Web API, REST, XML, JSON, Security (OAuth, OpenID Connect)  Ability to work with Linux OS to deploy and configure AXWAY gateway and other components  Solid hands-on design and development experience in TIBCO suite of BW, BPM, BE, EMS, Hawk, and Adapters etc.  Track record in providing application and technical assistance on multiple systems and platforms Understanding of XML file format, hashing and encryption, and transfer protocols (SFTP, NDM, etc.) Experience of Cloud based architecture and development Experience in Software delivery frameworks such as agile, waterfall Experience of CI/CD Core Competencies Solid software engineering experience Strong analytical and problem solving skills Technical depth across a number of technologies (languages, OS, Databases) Excellent written and verbal reasoning and communication skills Ability to lead technical solutions end to end Ability to work across organizational boundaries and build networks to deliver solutions ","Roles & Responsibilities Develop world-class solution/application for your team Be up to date of the market landscape for solution/application insights, direction, vendors, and methods Provides expertise to identify and translate system requirements into software design artefacts Provide input during the business development life cycle Participate in experimentation to assess new solution/application paths Identify challenges to help the development of formalized solution methodologies Contribute to a repository for solution/application artefacts Interface and coordinate tasks with internal and external technical resources. Collaborate to provision estimates, develop overall implementation solution/application plan, and serve as a lead as required, to implement the installation, customization, and integration efforts Actively contribute to the quality assurance for services within the solution/application area Provide relevant and timely project information to senior management Actively contribute to the change in delivery and deployment strategy for all applications to a total replacement for applications at the end of their technology or functionality lifecycle Maintain and monitor all aspects for the proper running of the application Understand the system process flow of the primary business processes. Provide a clear picture of the functionality map and the applications footprint of various applications across the map ","$6,500to$13,000",Monthly,Manager,data engineer
,,,,,,,,,,data engineer
"CHARLES & KEITH GRP HEADQUARTERS, 6 TAI SENG LINK 534101",CHARLES & KEITH (SINGAPORE) PTE. LTD.,Full Time,Information Technology,DATA ENGINEER,"RequirementsWe are looking for a candidate with 5+ years of experience in a Software Engineer role who wants to move into Data Engineering or is currently working as Data Engineer. He/She should have attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field and also have experience using the following software/tools:  Experience with object-oriented/object function scripting languages: Python and/or Java, C++, Scala, etc. Experience with big data tools: Hadoop, Spark, Kafka, NiFi, sqoop, etc. Experience with relational SQL and NoSQL databases. Experience with data pipeline and workflow management tools: Luigi, Airflow, etc. Experience with AWS cloud services: EC2, EMR, Kenesis, Firehose Experience with stream-processing systems: Storm, Spark-Streaming, etc. ​    Additional Requirements  Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with widely used RDBMS Experience building and optimizing ‘big data’ data pipelines, architectures and data sets Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement Strong analytic skills related to working with unstructured datasets. Build processes supporting data transformation, data structures, metadata, dependency and workload management A successful history of manipulating, processing and extracting value from large disconnected datasets Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment   ","Roles & ResponsibilitiesWe are looking for a savvy Software turned Data Engineer to join our growing Data Engineering team. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. He/She will be responsible for designing, expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Supporting our software developers, data architect, data analysts and data scientists on organisation wide data initiatives, he/she will ensure optimal data delivery that is consistent throughout ongoing projects. Self-directed and comfortable supporting the data needs of multiple teams, systems and products, he/she will be excited by the prospect of optimizing or even re-designing our organisation’s data architecture with architect to support our next generation of products and data initiatives. Roles & Responsibilities  Create and maintain optimal data pipeline architecture Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, collaborate with infra team to re-designing infrastructure for greater scalability and stability Collaborate with infrastructure team for provisioning required infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data needs Keep our data separated and secure across national boundaries through multiple data centres and AWS regions Create data tools for analytics and data scientist team members that assist them in building and optimizing models which enables us as an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems Build tools from ground up that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics     ","$5,000to$6,000",Monthly,Executive,data engineer
137 TELOK AYER STREET 068602,MATCHMOVE PAY PTE. LTD.,Full Time,Banking and Finance,Data Engineer (Fin-Tech),"Requirements Good programming experience in Python Knowledge of information retrieval using SQL, java or C++ Experience with AWS data technologies such as Redshift, Glue, Spectrum, Quicksight. Well versed with data modelling and data warehousing. Working knowledge of message queuing, stream processing, and highly scalable data stores. Knowledge about Apache Spark or Hadoop would be an advantage. Self-starter and committed to working in fast paced environment 	   Culture in MatchMove :   - To work in a fast-moving startup, fun and yet professional environment that recognizes and rewards individual contributions and also team success.  - To work with highly motivated people who are totally focused on winning by combining great teamwork, rapid execution and an uncompromising approach to quality and customer satisfaction.  - We strongly encourage Innovation, Collaboration, Creativity, and Initiative.  - We work in a collaborative environment where you can talk to the CEO anytime!  - Be A Part of the MatchMove Family! Check us out our Facebook page   Personal Data Protection Act : By submitting your application for this job, you are authorizing MatchMove to: a) Collect and use your personal data, and to disclose such data to any third party with whom MatchMove or any of its related corporation has service arrangements, in each case for all purposes in connection with your job application, and employment with MatchMove; and b) Retain your personal data for 1 year for consideration of future job opportunities (where applicable for relevant unsuccessful job applicants).","Roles & ResponsibilitiesAre you the One?  MatchMove Pay, one of the fastest, award-winning Fin-Tech company, is looking for a couple of experienced Data Engineers to work on in-house data warehouse projects and data modelling.  Job Responsibilities :  Build the data warehouse infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources on AWS technologies. Maintain compliance of the data warehouse with MatchMove data architecture policy. Responsible for data modelling and validation of master data with original data sources. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability Work with stakeholders including the Executive,  Finance, Product and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. 	  ","$3,500to$5,000",Monthly,"Manager, Senior Executive",data engineer
"KEPPEL TOWERS, 10 HOE CHIANG ROAD 089315",EIRE SYSTEMS SINGAPORE PTE. LTD.,Full Time,Information Technology,Data Security Network Engineer,RequirementsExperience in the below Technology The primary technology areas covered by the Data Security Engineer are:  Juniper Netscreen Firewalls and Netscreen Security Manager Palo Alto Firewalls and Panorama CM Cisco Routing and Switching (Cisco Nexus) Multicast EIGRP and BGP Routing Protocols Lucent Vital QIP/Run IP (DNS/DHCP); N3K RunIP F5 Load Balancers Bluecoat Proxy Devices Rebasoft Network Access Control  The preferred candidate should have worked in a System Integrator/ large enterprise managed service environment preferably in the financial industry or have similar experience with the banking sector.,"Roles & ResponsibilitiesThe role of an Implementation Services Data Security Engineer is to interpret a customer request and then transform that request into a successful project.  This includes project management, solution design, security solution implementation as well as act as Tier 3 escalation technical support when called upon.  The role is required to act as liaisons to engineers in the Client Architecture and Engineering department in ensuring that global engineering standards and guidelines are adhered to.   Responsibilities include: Requirements verification:  Meeting with Business Clients, Systems Integration, and other IT departments to gather technical requirements, business justification for projects Verify technical requirements for large projects Quickly estimate high-level costs and resource requirements  Solution design:  Evaluating technology options and presenting those that best suit customer requirements. Produce the technical design in the format specified by Client standards.  This requires excellent technical writing skills and proficiency with office automation tools.  Many technical designs will be in excess of 100+ pages and integrate various items including Visio Diagrams, Tables, Charts, and links. Ensure Compliance with all Client Standards for Data and Voice Networks. Assist junior engineers with project costing estimates Generally the first to use new solutions provided by Network Technology engineering team    Project Management  Develop a High-level plan at project initialization that will guide the project through the requirements and pre-sales stage Develop a Detailed project plan that will guide the project through the solution design, procurement, and delivery stages. Hold regular meeting with stake holders to track progress and communicate issues, and delegate tasks Provide Project reporting as required by the Client Minimal Grid Guidelines for Complex and Medium Service Requests  Implementation:  Determining implementation time frames and risks - Coordinating and gaining consensus on change windows with change management and other IT teams and LBM’s Submission and tracking of Change Management tickets for implementation Coordinating the installation of equipment and connections with Move/Add/Change team Testing of network connectivity post implementations Ensuring Compliance of all Changes with Client standards Completing the Acceptance into Service (AIS) Process for all projects. Delivering all Projects and Changes “Right the First Time” as most of our SLA for service uptime are 99.999% and greater.  Failure to deliver right the first time has significant financial penalties. Travel to various customer sites within the Asia Pacific theater is required    General Support:  Provide Tier 3 Technical Support during major incidents and troubleshooting assistance for complex long term issues impacting network performance. Provide Technical consultation as requested by the customer Assist with internal and external audit requests   ","$7,000to$9,000",Monthly,Professional,data engineer
,ALPHATECH BUSINESS SOLUTIONS PTE. LTD.,Permanent,Information Technology,Data Quality Engineer,"Requirements   Must know JAVA8 and SPARK Experience in distributed data architecture Have working knowledge of SQL, Python, Airflow Scala, Hadoop, SPARK Good to know CI/CD Experience (Jenkins Github), AWS, Kubernetes, Docker Preferred to have banking domain experience ",Roles & Responsibilities Evaluate and recommend solutions via data analysis regarding issues related to the improvement of product qua;oty and resolving of customer feedback Apply software and programming abilities to manage and analyse data from a variety of sources ,"$6,000to$8,000",Monthly,Senior Executive,data engineer
"MARINA BAY FINANCIAL CENTRE, 12 MARINA BOULEVARD 018982",DBS BANK LTD.,Full Time,Engineering,"VP, Lead DevOps Engineer, Group Consumer Banking and Big Data Analytics Technology, T&O (180001X5)","Requirements More than 3 years of experience as a Developer Excellent problem solving skills Excellent communication skills in order to facilitate workshops Strong knowledge and experience in Devops automation, containerisation and orchestration using tools such as Mesos Chef, Ansible, Docker, Jenkins, SonarQube Kubernetes etc. Experience with highly scalable distributed systems Hands on in depth experience in some of the following technologies: Jenkins/Maven/Git/SonarQube/Fortify/Confluence/Jira/Artifactory Cloud Foundry, OpenShift or other PaaS technologies. Public clouds such as AWS, Google Cloud or Azure. Dockers, Garden, Kubernetes, Mesos. Strong understanding of virtualization and networking. Strong understanding of Linux. Familiarity with relational databases, preferably MySQL, NoSQL, MariaDB, PostgreSQL. Experience working with, or an interest in Agile Methodologies, such as Extreme Programming (XP) and Scrum  Knowledge of software best practices, like Test-Driven Development (TDD). ","Roles & ResponsibilitiesBusiness Function  Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.   Key Accountabilities  Manage the development of the internal engineering productivity tools and environments. Providing DevOps architecture implementation and operational support Architecture and planning for cloud deployments (Private and Public cloud);  Be an innovative and hands-on DevOps engineer capable of looking at both the technology and strategy around the platform. Future-proofing the technical environments and ensuring extreme high levels of automation, availability, scalability and resilience.  Responsibilities   Manage the development of the internal engineering productivity tools and environments. Manage processes, automation, best practices, documentation. Development and operation of continuous integration and deployment pipelines. Monitoring automation to effectively detect/predict/prevent issues in the environment and code base. Ability to conduct research into software issues and products as required Working with the latest tools and techniques  Hands-on coding and mentoring, usually in a pair programming environment  Working in highly collaborative teams and building quality environments. Ability to effectively prioritize and execute tasks in a high-pressure, fast paced, global environment Knowledge in lots of different open source technologies and configurations. ","$9,500to$15,000",Monthly,Senior Management,data engineer
"AXA TOWER, 8 SHENTON WAY 068811",LAZADA SERVICES SOUTH EAST ASIA PTE. LTD.,Permanent,"Information Technology, Logistics / Supply Chain","Vice President, Data Engineering","Requirements  Minimum 5 years of data engineering experience.   Experience in leading more than 2 people in team.   Expert/Advanced level experience with Python.   Expert Level experience with PostgreSQL, Hadoop.   Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases.   Experience or understanding of Big Data Platforms.   eCommerce industry knowledge is a plus.   Great experience building production applications in a heterogeneous environment.   Experience with Multithreaded and Concurrent programming.   Creative and nimble with ability to overcome obstacles to solve the hardest problems   Ability to write well-abstracted, reusable code components (TDD / Git / Jenkins / Ansible as plus).   Experience in cloud computing services, relational, and non relational databases for business intelligence and analytics.  ","Roles & ResponsibilitiesStory of Lazada Group: Launched in 2012, Lazada has grown rapidly to include over 4.900 full-time employees in the region, with eCommerce operations in Indonesia, Malaysia, Philippines, Singapore, Thailand, Vietnam and a sourcing center in Hong Kong that drives cross-border marketplace activities as well as an R&D TechHub in Russia. Revolutionizing the way customers shop in Southeast Asia and perform online transactions across the region, Lazada has reached an online footprint of approximately 9 million unique daily visits to its websites, and the largest Facebook community in Southeast Asia with over 16.5 million fans. Lazada Group owns the biggest and the most efficient technology driven logistics and fulfilment ecosystem in the region – Lazada eLogistics. With 11 own warehouses, 5 sorting centers, 78 last-mile hubs we are ensuring 48 hours delivery of more than 6 million orders every month. Our warehouses cover more than 115 thousands of square miles and it takes less than 2 hours to process every order even during massive sales campaigns. Having our own cross-border operator helps us connect more than 100 million of customers and businesses from all over Asia. Our transportation is driven by our own LEL Express delivery fleet which, together with more than 80 third-party logistics, guarantees high quality 48 hours delivery. All of that would be impossible without sophisticated IT systems, which are being developed and expanded in-house by one of the most experienced and agile tech teams in Southeast Asia! As a Lead Data Engineer in Lazada eLogistics Tech Team, you'll be part of an extremely motivated and experienced group of people. You'll help drive LEL business and be a key contributor. You will also become a mentor for other developers and business members. Does the real-time challenge of dealing with massive datasets (billions of transactions a day) get you excited? If yes, then we would like to speak with you. Lazada eLogistics is using a mix of cutting edge and proven technologies to build new data products that aim to change the E-Logistics landscape. You will be the tech leader of a data engineering team that primarily focuses on productionalizing data pipelines that drive our most critical applications. The tech lead position is the a critical layer that makes sure projects get done. Your daily duties will be:   Drive development of real-time data ingestion pipelines and batch data ingestion pipelines for analysis, machine learning, dashboards, alerts and visualizations.   Drive development of new systems and tools to enable data scientists to consume and analyse data faster and more efficiently.   Design data warehouse and data pipelines ensuring data integrity between systems are maintained.   Architect, build, and launch new data models.   Execute code review of data engineers.   Mentor data engineers in the team.   Convert specs into to working code.   Work and tune data warehousing and data ingest environments.   Script programs and APIs in Python/Go.   Create, monitor and manage low latency ETL and Data pipelines.   Your future benefits will be:    Class ""A” office with the best view on business district of Singapore.   Official employment and relocation coverage.   Medical insurance from the first day.   Comfortable working hours in the office.   Caring and respectful HR team.   Powerful workstations and various software licenses (Mac / Winbook + HD displays to your liking).   Daily snacks, chill-out on Friday and of course high quality coffee.   Personal development system for both specialists and managers.   Choice of hackathons, meetups and other entertainment activities.   Opportunity to become public speaker in technology and take part in industry conferences – for top performers.   Exciting international business travels.   No dress code.  ","$13,000to$17,000",Monthly,"Senior Management, Middle Management",data engineer
,Company Undisclosed,Full Time,"Engineering, Information Technology, Others",Data Engineer (Big Data),"Requirements You have 10+ years hands-on experience and a passion for data engineering in a big data envrionment, ETL Processes and related services as well as experience and a very good understanding of AWS Big Data Cloud Infrastructure. You have very good experience in relational Database Technologies (ideally MS SQL) and Big Data Technologies and Database Performance Tuning . You are a highly motivated, challenge-taking Personality with a positive attitude. Team-oriented with an international mindset. You like to work in a dynamic Start-Up Environment and build exciting new Data Products within a motivated team. Fluent in English written and oral and ready to travel. You feel comfortable in developing on your own but also in working with a team of developers   ","Roles & Responsibilities Being part of the Data Engineering Team to conceptualise, build and maintain the Data Infrastructure for relational as well as Big Data Infrastructure in order to deliver a high performanc Data Environment to process Retail and Shopper Data from various partners. Being responsible for Data Warehousing Design, Data Integration Processes, Database Ressource Planning, Infrastructure Performance, Data Governance and Security Management as well as design, architecture, implementation and documentation of new, scalable ETL processes, pipelines, pathways and dimensional data models. Audit and QA data and processes to ensure data quality and integrity throughout the data ecosystem Work with Data Scientist, Business Consultant and Frontend Developer to build scalable Data Analytics Apps using advanced analytics.     ","$6,000to$8,000",Monthly,Executive,data engineer
"KEPPEL TOWERS, 10 HOE CHIANG ROAD 089315",ZALORA SOUTH EAST ASIA PTE. LTD.,Full Time,Engineering,Data Engineer,"Requirements BS in Computer Science or related technical discipline or equivalent practical experience 3-4+ years of experience with high-traffic, high volume, high scalable distributed systems and client-server architectures (clustering, partitioning, sharding, etc) Some experience working with Data Scientists and finding solutions for them to work efficiently while manipulating high volume of data and be able to work with them and the teams to bring their algorithms at scale Strong operational experience with AWS, container approaches. ","Roles & ResponsibilitiesWe are looking for a Big Data Engineer happy to design, build, maintain and automate big data environments (datalake, etc…) and the associated data to enable the teams to make use of the high volume of data available from our e-commerce activities. You should be proficient in: - Technical background:  Linux Big Data technologies (Redshift, BigQuery, Spark, Glue, Parquet…) Industrialization (Ansible..), Orchestration (Kubernetes…), containers in cloud (Docker, AWS…) Strong experience in resilient architecture (high availability, scalability) Data management: you have experience in integrating and managing large volumes of Data while taking into account performance issues Coding skills: skills in one or more scripting languages (Perl, Ruby…) as well as one or more development languages (Python, Java…)  Soft skills: while being a tech automation enthusiast with a passion for building tools to make developers' lives easier, you also want and know how to share your expertise with other people to empower them. Agile and DevOps approach, with an operational experience as an Ops in a demanding environment. You know what it’s like to manage in production critical systems and you have experience in sharing this knowledge to the teams to enable a “you build it / you run it” mindset.","$4,500to$6,000",Monthly,Professional,data engineer
"INTERNATIONAL PLAZA, 10 ANSON ROAD 079903",SMARTSOFT PTE. LTD.,Full Time,Information Technology,senior Big data Engineer,"Requirements Commercial software engineering: You have 3+ years of professional software development experience with languages and systems such as Java, Python (PySpark), and version control (git), with good analytical & debugging skills. Big data: You have extensive experience with data analytics and working knowledge of big data infrastructure such as Hadoop Eco System, HDFS, Spark, Google Cloud, Big Query, Data Flow (nice to have). You've routinely built data pipelines with gigabytes/terabytes of data and understand the challenges of manipulating such large datasets. Data Modeling: Flair for data, schema, data model, PL/SQL, Star & snow flake schema, how to bring efficiency in data modeling for efficient querying data for analysis, understands criticality TDD and develops data validation techniques. Real Time Systems: Understands evolution of databases for in-memory, NoSQL & indexing technologies along with experience on real-time & stream processing systems like kafka, Storm. Project management: You demonstrate excellent project and time management skills, exposure to scrum or other agile practices in JIRA. ","Roles & Responsibilities Massive data: You will source / examine, analyze, engineer data pipelines for gigabytes/terabytes of structured and unstructured data with our platform to create value for customers. Pushing the limits: This role will be on the cutting edge of our Data / Machine Learning platform. As we push to solve more of our customer challenges, you will be prototyping new features, tools and ideas. Innovate at a very fast pace to maintain our competitive edge. Linux hacking: You will be masterfully using the command line, including tools like vi/emacs and understanding beyond basics of grep, bash, awk, sed, etc to aggressively dive into data, systems, and compute platforms to get the results you are seeking. Production deployment: You will be responsible for integration and deployment of the machine learning pipelines into production where your ideas can come to life. Coordinate and work with cross functional teams, sometimes located at different geo locations. ","$4,000to$8,000",Monthly,Senior Executive,data engineer
,DENODO TECHNOLOGIES PTE. LTD.,Full Time,Engineering,Data Engineer - SQL  /  Big Data  /  Java,"RequirementsQualifications Required Skills  BS or higher degree in Computer Science. Solid understanding of SQL and good grasp of relational and analytical database management theory and practice. Knowledge in Java software development, especially in the database field. Good knowledge of JDBC, XML and Web Services APIs. Excellent verbal and written communication skills to be able to interact with technical and business counterparts. Active listener. Strong analytical and problem solving abilities. Lots of curiosity. You never stop learning new things. Creativity. We love to be surprised with innovative solutions. Willingness to travel around 50%. Be a team worker with positive attitude.  We Value  Experience working with Java frameworks. Experience working with GIT or other version control systems. Experience working with BigData and/or noSQL environments like Hadoop, mongoDB, ... Experience working with caching approaches and technologies such as JCS. Experience in Windows & Linux (and UNIX) operating systems in server environments. Business software implementation and integration projects (e.g. ETL/Data Warehouse architectures, CEP, BPM). Integration with packaged applications (e.g. relational databases, SAP, Siebel, Oracle Financials, Business Intelligence tools, …). Industry experience in supporting mission critical software components. Experience in attending customer meetings and writing technical documentation. Foreign language skills are a plus.  Additional Information Employment Practices  We are committed to equal employment opportunity. We respect, value and welcome diversity in our workforce. We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee. ","Roles & ResponsibilitiesYour Opportunity Denodo is always looking for technical, passionate people to join our Services Engineering team. We want a professional who will travel, consult, develop, train and troubleshoot to enhance our clients’ journey around Data Virtualization. Your mission: to help people realize their full potential through accelerated adoption and productive use of Denodo solutions. In this role you will successfully employ a combination of high technical expertise and client management skills to conduct on-site and off-site consulting, product implementation and solutions development in either short or long-term engagements being critical point of contact for getting things done among Denodo, partners and client teams. Duties & Responsibilities  Obtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical pitch, including overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning. Constantly learn new things and maintain an overview of modern technologies. Be able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature / functionality of our product. Capable of building and/or leading the development of custom deployments based and beyond client’s requirements. Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues. Train and engage clients in the product architecture, configuration, and use of the Denodo Platform. Promote knowledge and best practices while managing deliverables and client expectations. Manage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues. Provide technical consulting, training and support. Develop white papers, presentations, training materials or documentation on related topics ",,,Professional,data engineer
51B CIRCULAR ROAD 049406,PALO IT SINGAPORE PTE. LTD.,"Permanent, Full Time",Information Technology,Senior Database Consultant - Big Data Engineer,"Requirements✔     You hold a Bachelor, Master or PhD degree in IT, Information Management and/or Computer Science ✔     You are just graduated or have less than 3 years of working experience ✔     Good knowledge of big data technology landscape and concepts related to distributed storage / computing ✔     Experience with big data frameworks (e.g. Hadoop, Spark) and distributions (Cloudera, Hortonworks, MapR) ✔     Experience with batch & ETL jobs to ingest and process data from multiple data sources ✔     Experience with NoSQL databases (e.g. Cassandra, MongoDB, Neo4J, ElasticSearch) ✔     Experience with querying tools (e.g Hive, Spark SQL, Impala) ✔     Experience or willingness to go in real-time stream processing, using solutions such as Kafka, Flume and/or Spark Streaming ✔     You are passionate about technology and continuous learning comes naturally to you  ","Roles & ResponsibilitiesYour profile & role on the project YOU:  Thrive on challenge. When was the last time you failed? Are curious & always learning. What are you up to right now? Can deal with constant change. When were you last surprised? Have mastered at least one skill of your trade but you’re not defined by it. What can you teach us? Can you wear many hats?  YOU AGAIN: The DevOps Architect will install, maintain, and support an on-premises cloud infrastructure and apply DevOps practices and solutions. The person will also implement cloud-related and DevOps technologies such as AWS/Puppet/Chef/Elk/Azure/Openstack. Other infrastructure related activities such as maintaining the company internal server infrastructure and respond to consultant requests when required will be expected.  Install, maintain, and support on-premises and off-premises cloud stack. Configure, maintain, and support the cloud-related infrastructures. Act as a system administrator on different OSes (e.g. RHEL, Opensolaris, Ubuntu, etc.) and help teams deploy their application and automate their development and releases on the cloud. Ability to develop solutions and self-learn new tools and technologies. Document, and share knowledge on developed DevOps solutions.  STILL YOU:  Unix / Linux / Bash knowledge Very good understanding of cloud computing (e.g. Technologies, Deployment, costing, HA/DR, etc.) Good understanding of DevOps principles (e.g. testing automation, BDD, TDD, Release automation, CI/CD, etc.) 2 years experience with cloud deployment (e.g. Openstack, VMWare, AWS, Azure, Terraform, etc.) 1 year experience with testing automation (e.g. Maven, Selenium, HP QC, LoadRunner) 1 year experience with release automation process (e.g. CA-RA, Jenkins, etc.) 1 year experience with Configuration Management (e.g. Ansible, SaltStack, Puppet/Chef, etc.) 1 year experience with monitoring tools (e.g. ELK, Prometheus, Grafana and Splunk) Experience with developing and implementing processes to handle releases from Development to Operations while respecting internal rules, and offering solutions for rollback) Experience with designing an architecture to implement development-to-production workflows. Knowledge of SRE, Containers, Kubernetes, Openshift is a plus. Good understanding of microservice architecture and DevOps practices that support. Strong RDMS and NoSQL skill in deploying and fine tuning such as MySQL, Oracle, Elasticsearch.  Your role at PALO IT You will be invited to take part in R&D works done within our Practices. You will have the chance to assist or be a speaker at must-attend international IT conferences. You will have the opportunity to write articles for our Blog or specialized press. Genuine ambassador of PALO IT, you will present our offers and take an active role in the development of the company.  Your technical environment # Cloud and DevOps based technologies (AWS/Puppet/Chef/Elk/Azure/Opencloud) # DevOps practices # Linux OS, Shell Scripting, SQL # Agile and scrum environment","$6,000to$12,000",Monthly,Professional,data engineer
"SUNTEC TOWER FOUR, 6 TEMASEK BOULEVARD 038986",AVIVA ASIA PTE LTD,Permanent,Information Technology,"Manager, Data Engineer","RequirementsQUALIFICATIONS  Bachelor degree or above Masters (preferred)  KNOWLEDGE/EXPERIENCE ·A deep understanding of Data Modelling best practices  Good understanding of  tools and use in a commercial environment Preferably of 2-5 years of experience working in Hadoop/ Big Data related field Must possess working experience on Hive, Spark, HBase, Sqoop, Impala, Kafka, Flume, Oozie, MapReduce etc Working experience on ETL tools like Oracle PLSQL, Informatica etc Deep understanding of the Hadoop ecosystem and strong conceptual knowledge in Hadoop architecture components Possess data management, data visualization and statistical analysis experience Self-starter who works with minimal supervision. Ability to work in a team of diverse skill sets Experience working in Agile development process and has good understanding of various phases of Software Development Life Cycle Good interpersonal with excellent communication skills -written and spoken English    SKILLS Required  Data Warehouse Technologies Hadoop Proficiency Proficient in Hive, Spark, HBase, Sqoop, Impala, Kafka, Flume, Oozie, MapReduce etc. Working experience on ETL tools like Oracle PLSQL, Informatica etc data management, data visualization and statistical analysis skills    Desirable  Experience in working in Cloud environment   ","Roles & ResponsibilitiesPURPOSE AND CONTEXT OF THE ROLE Taking leadership in:  Design and develop architecture for data services ecosystem spanning Relational and Big Data technologies Design data models for mission critical and high volume data management, real-time and distributed data process aligning with the business requirements. Work with business units on their analytics initiatives, supply and/or source analytics expertise and resources. Ensure the appropriate technology resourcing and support for the Data Engineering and analytics team.    Lead projects involving high level of coordination among departments and business areas.   OUTCOMES  Produce optimal solutions in the receipt and delivery of data sets to desired destinations Alignment to Data Strategy, Digital Strategy, IT Strategy, Architecture and Transformation roadmap Maintain IT Applications Development Excellence amongst IT Development community through detailed development and training and the conduct of Knowledge transfer for completed tasks Produce Optimal solutions, through detailed Impact Analysis, Technical Solutions, Technical Specifications and provide Leadership in Projects, Change Initiatives and Solution Delivery. Supervise IT teams to produce and complete assigned tasks for Strategic, Mandatory and Tactical Change Request, within budget and within projected Scope of work.   ","$10,000to$12,000",Monthly,Manager,data engineer
,FINSURGE PTE. LTD.,Full Time,Information Technology,senior data engineer,"RequirementsTechnical background in computer science, data science, machine learning, artificial intelligence, statistics or other quantitative and computational science 5+ years of experience with a proven track record of building scalable and performant data infrastructure, implementing data warehousing and business intelligence projects: MS SQL Server, Oracle, MySQL, PostgreSQL Delivering production ETL jobs using Informatica stack including: Power Center, PowerExchange, Informatica Data Explorer Experience with the following tools is a bonus: Big Data Management, Big Data Streaming, Enterprise Data Integration, Enterprise Data Lake, Enterprise Data Catalog, Customer 360 Experience with insert other Temasek ETL tools e.g. SSIS is a bonus Experience implementing Data Quality (DQ) framework is a bonus: Data profiling DQ validation rules Automatic cleansing 3+ years of experience with Hadoop architecture and components including: Hive Spark Kafka Nifi Sqoop HDFS architecture HBase MapReduce Experience with NoSQL databases is a bonus: MongoDB, Cassandra, etc. Experience building APIs is a bonus Experience with the following languages: SQL Bash HiveQL Scala, SparkSQL, PySpark Python Java","Roles & ResponsibilitiesWe are looking for a Senior Data Engineer who will join the Digital Technology Team as we are at an early developmental stage and planning for considerable growth over the next 12 months. We need an experienced data engineer to design and develop data infrastructure. As we are looking to build the data pipeline from scratch, you will have autonomy and the technical backing from our engineering team in designing,developing and maintaining this infrastructure. Contribute to key data pipeline architecture decisions and lead the implementation of major initiatives Translating business requirements into technical specifications and documentation Developing code standards, ETL architecture standards, and naming conventions Designing, executing and documenting ETL testing plans Optimizing performance of ETL jobs Develop the team’s data capabilities - share knowledge, enforce best practices and encourage data-driven decisions.","$7,000to$8,500",Monthly,Senior Management,data engineer
"AXA TOWER, 8 SHENTON WAY 068811",LAZADA SOUTH EAST ASIA PTE. LTD.,Permanent,Information Technology,"Senior Manager, Data Engineer",Requirements Educational background in Computer Science / Electrical Engineering or similar Understanding of database concepts and distributed processing systems Experience with programming and understanding of basic algorithms Strong command of SQL and database concepts Preferred previous knowledge of Hadoop or NOSQL databases Excellent communication & problem solving skills  ,"Roles & ResponsibilitiesTeam Introduction Lazada is the number one online shopping & selling destination in Southeast Asia – present in Indonesia, Malaysia, the Philippines, Singapore, Thailand and Vietnam. Lazada helps more than 80,000 local and international sellers as well as 2,500 brands serve the 560 million consumers in the region through its marketplace platform, supported by a wide range of tailored marketing, data, and service solutions. Lazada offers an excellent customer experience through a wide network of logistics partners and its own first- and last-mile delivery arm. Roles & Responsibilities  Implementing working data projects based on given technical specifications Develop real-time and batch data ingesting and processing pipelines to be used for analysis, machine learning, dashboards, alerts and visualizations. Work closely with partner teams to establish the optimal technical solution to business problems Monitor & manage data pipelines, ensuring accuracy and stability ","$7,000to$10,500",Monthly,"Middle Management, Manager",data engineer
,ABAKUS (ASIA PACIFIC) PTE. LTD.,Permanent,"Banking and Finance, Information Technology",Data Engineer,"Requirements Experience working on Big Data technologies such as Spark, Redshift etc Good working experience with Kubernetes or containers in cloud such as Docker or AWS Strong experience in resilient architecture (high availability, scalability) Experience in integrating and managing large volumes of Data while taking into account performance issues Skilled in Python, and experienced with packages related to machine learning and data science (e.g. pandas, numpy, matplotlib, scikit-learn) Strong foundation in computer science and software engineering, and ability to deliver and test production level code Bachelor in Computer Science or related technical discipline or equivalent practical experience At least 5 years of experience with highly scalable distributed systems and client-server architectures (clustering, partitioning, sharding, etc) Experience working with Data Scientists and finding solutions for them to work efficiently while manipulating high volume of data and be able to work with them and the teams to bring their algorithms at scale Passion in building tools, empower developers practicing Agile and DevOps approach, with operational experience in a fast pace and demanding environment.  We're building an amazing team and are constantly on the lookout for like-minded and motivated individuals to join our fast-paced and challenging environment, so as to contribute to our mission to improve financial inclusion and change the lives of Southeast consumers.","Roles & ResponsibilitiesTo reinvent an industry, you need to build an all-star team. Join Wecash if you want to leverage upon the power of big data and machine learning to develop and promote products that can provide businesses with better credit profiles of customers and underwrite loans between funding sources and consumers. Founded in 2014, we are the first Chinese startup using big data and machine learning to evaluate consumer credit and detect fraud. Our company has raised more than US$200 million in financing over 4 rounds, acquired over 130 million users and have underwritten over Billions of USD loans to transform the lifestyle and credit worthiness of individuals over the past 4 years. We are looking for a stellar technologist to drive our expansion in South Asia. If you can understand complex technology, navigate the fintech industry and thrive under ambiguous objectives, join us as our Senior/Lead Data Engineer for Southeast Asia, and help us grow.","$4,000to$6,500",Monthly,"Executive, Senior Executive",data engineer
"YELLOW PAGES BUILDING, 1 LORONG 2 TOA PAYOH 319637",HELIX LEISURE PTE. LTD.,Permanent,Information Technology,"Software Engineer, Data Services","RequirementsSkills & Qualifications:  BS degree in Computer Science preferred, similar technical field of study or equivalent practical experience will be considered Strong Core Java Development Experience Experience working with three or more from the following: web application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, and/or security software development Working proficiency and communication skills in verbal and written English Strong TSQL knowledge, able to optimise queries and understand unoptimised query plans Real Time, Multithreaded experience Experience with ORM tools such as hibernate Experience building high-volume file processing systems Experience with payments/transactions Experience in an Agile development environment. DBA experience Effective teamwork and good communication skills with the ability to mentor peers and provide peer code-reviews Ability to work effectively with software engineers to enhance test plans and automated testing framework ","Roles & ResponsibilitiesHelix Leisure is a leading global supplier to the Out of Home Entertainment industry – locations outside the home people visit for entertainment and recreation. Across our core brands – Embed (revenue management systems, e-commerce), Booking Boss (Tours, Attractions and Activities), LAI Games (arcade games), The Locker Network (operating electronic lockers) and Matahari Leisure (equipment manufacturing) we service over 2,500 locations around the globe. Helix operates full service offices in Singapore, Perth, Sydney, Dallas, Dubai and Jakarta. The group enables our customers to create rich experiences for their visitors and guests through both technology and service. As we embark on building the next generation platform for our software – a core consumer, supplier and distributor facing application, we are looking for highly motivated professionals who enjoy working in a fast paced, agile development environment. You will be working closely with product owners and UX designers to create and develop best-in-class data service solutions with the ability to use the latest in web development technology. Responsibilities:  Design, develop, test, deploy, maintain and improve software Manage individual project priorities, deadlines and deliverables. ","$5,000to$10,000",Monthly,Senior Executive,data engineer
"UOB PLAZA, 80 RAFFLES PLACE 048624",A*STAR RESEARCH ENTITIES,"Contract, Full Time",Sciences / Laboratory / R&D,Research Engineer (Data Analytics)  /  I2R (A*STAR),"Requirements Minimum Bachelor degree in the field of computer science, computer engineering, mathematics and statistics, electrical engineering, or other data science intensive program. With expertise in at least one of the following areas: data mining and management, machine learning, statistical learning, time-series analytics Possess minimum 1 year of relevant work experience Ability to work independently to translate research ideas into programs with efficient coding Basic knowledge on data analytics, machine learning, data mining Proficient in Python, R, C++ or Java Prior industry experience with engineering, financial services, healthcare, or urban development is a plus Able to deliver under tight schedule Good team player with both research and engineering ethics Good interpersonal and communication skills  The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.","Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  ","$2,500to$5,000",Monthly,Professional,data engineer
"UOB PLAZA, 80 RAFFLES PLACE 048624",A*STAR RESEARCH ENTITIES,"Contract, Full Time",Sciences / Laboratory / R&D,Research Engineer (Data Analytics)  /  I2R (A*STAR),"Requirements Minimum Bachelor degree in the field of computer science, computer engineering, mathematics and statistics, electrical engineering, or other data science intensive program. With expertise in at least one of the following areas: data mining and management, machine learning, statistical learning, time-series analytics Possess minimum 1 year of relevant work experience Ability to work independently to translate research ideas into programs with efficient coding Basic knowledge on data analytics, machine learning, data mining Proficient in Python, R, C++ or Java Prior industry experience with engineering, financial services, healthcare, or urban development is a plus Able to deliver under tight schedule Good team player with both research and engineering ethics Good interpersonal and communication skills  The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.","Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  ","$2,500to$5,000",Monthly,Professional,data engineer
,,,,,,,,,,data engineer
"UOB PLAZA, 80 RAFFLES PLACE 048624",A*STAR RESEARCH ENTITIES,"Contract, Full Time",Sciences / Laboratory / R&D,Research Engineer (Data Analytics)  /  I2R (A*STAR),"Requirements Minimum Bachelor degree in the field of computer science, computer engineering, mathematics and statistics, electrical engineering, or other data science intensive program. With expertise in at least one of the following areas: data mining and management, machine learning, statistical learning, time-series analytics Possess minimum 1 year of relevant work experience Ability to work independently to translate research ideas into programs with efficient coding Basic knowledge on data analytics, machine learning, data mining Proficient in Python, R, C++ or Java Prior industry experience with engineering, financial services, healthcare, or urban development is a plus Able to deliver under tight schedule Good team player with both research and engineering ethics Good interpersonal and communication skills  The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.","Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  ","$2,500to$5,000",Monthly,Professional,data engineer
"UOB PLAZA, 80 RAFFLES PLACE 048624",A*STAR RESEARCH ENTITIES,"Contract, Full Time",Sciences / Laboratory / R&D,Senior Research Engineer (Data Analytics)  /  I2R (A*STAR),"Requirements Minimum bachelor degree in computer science, computer engineering, mathematics and statistics, data science intensive programs, with expertise in one or more of the following areas: data mining and management, machine learning, time-series data analytics, etc. Minimum 2 years post completion of last degree Experience in clinical research environments is a plus Excellent knowledge of a programming language such as Node.js., java or C++  Proficient in Python Good knowledge on data analytics/ machine learning/ data mining and experience in solving real-world data science problems Able to deliver under tight schedules Good team player with both research and engineering ethics Good interpersonal and communication skills Prior experience with NLP is a big plus Prior experience with medical image processing, clinical informatics systems and software platforms is a big plus  The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.","Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg The project focuses on data management systems, data engineering solutions and deep learning systems for radiology applications. Candidate should have demonstrated interests or experience in: 1. Experience in the execution of translational projects focused on development, testing and deployment  2. Big data analytics and data engineering 3. Experience with biomedical datasets, in particular medical images 4. Business analysis skills and/or past work with/in clinical partner institutions  The position entails working in a multi-disciplinary business analytics translation group alongside machine learning and deep learning teams that are closely collaborating with clinical and industry partners on impactful projects that will translate research to deployed technology.","$3,400to$6,800",Monthly,Professional,data engineer
"UOB PLAZA, 80 RAFFLES PLACE 048624",A*STAR RESEARCH ENTITIES,"Contract, Full Time",Sciences / Laboratory / R&D,Research Engineer (Data Analytics)  /  I2R (A*STAR),"Requirements Minimum Bachelor Degree in computer science or other related fields  Minimum 2 years experience in data analytics related projects   Well-versed in programming (Python, R, Java, J2EE, or C/C++), database (MySQL, NoSQL,MongoDB)   Experience in system development lifecycle   Familiarity with big data analytics toolkits/frameworks such as Spark/Hadoop/Cassandra/Mahout   Good team player, able to multitask and work independently   Good interpersonal and communication skills   The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.","Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R)  The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualisation solutions. For more information about I²R, please visit www.i2r.a-star.edu.sg We are looking for a capable and responsible engineer to work on and make contributions to a major big data R&D project on fraud risk prediction. The work scope involves design and implementation of big data management, analytics and web applications. Successful candidate will work with other research team members to do part of system implementation, data pre-processing, analysis and visualization. Successful candidate will also have opportunities to be involved in other industry projects and/or research projects.  ","$2,500to$5,000",Monthly,Professional,data engineer
"UOB PLAZA, 80 RAFFLES PLACE 048624",A*STAR RESEARCH ENTITIES,"Contract, Full Time",Sciences / Laboratory / R&D,Research Engineer (Data Analytics)  /  I2R (A*STAR),"Requirements Minimum Bachelor Degree with knowledge or exposure to Computer Science, Bioinformatics, Computational Biology, Statistics, or other Data Science intensive fields Candidates should have particular experience in one or more of the following areas:      Large-scale data handling and/or databases Experience with biomedical data analysis Genomics/ Computational and Systems Biology Medical Imaging Biomedical informatics/ Healthcare Data Analytics Knowledge extraction and feature engineering Natural Language Processing/Text Mining Exposure to machine learning and deep learning methods is highly encouraged   Experience in corporate or application oriented environments is a plus. Ability to work independently and as part of a multidisciplinary team  Quick learner, willing to acquire the necessary domain knowledge  Good communication skills for proposals, reports, and publications Proficiency in spoken and written English. Experience with biomedical and healthcare datasets (EMR, medical imaging, genomics) is a plus. Strong programming abilities (Eg: Python, R, MATLAB, C/C++, Java, Perl, Bash) Familiarity with data preprocessing, data science and data visualization tools (Eg: SAS, Tableau, Knime, WEKA, Jupyter notebooks, deep learning, machine learning and visualization libraries)  Bioinformatics applicants looking to switch into fields related to analytics and intelligent systems with significant domain expertise and strong programming skills will also be considered.    The above eligibility criteria are not exhaustive. A*STAR may include additional selection criteria based on its prevailing recruitment policies. These policies may be amended from time to time without notice. We regret that only shortlisted candidates will be notified.","Roles & ResponsibilitiesAbout the Institute for Infocomm Research (I²R)  The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualisation solutions. For more information about I²R, please visit www.i2r.a-star.edu.sg The project focuses on development of machine learning, deep learning and artificial intelligence algorithms for applicants in precision medicine. Candidates should have demonstrated interests or experience in one or more of the following:  Analysis of large scale heterogeneous biomedical datastreams (genomics, EMR, imaging, lifestyle) Projects involving biomarker identification, knowledge discovery, predictive analytics for patient outcomes, and/or clinical application. R&D for advanced algorithms.  Core responsibilities include preprocessing of raw disparate biomedical datasets, development of automated knowledge extraction and feature engineering pipelines, and design of pilot studies/demos. The position entails working in a multi-disciplinary machine learning and deep learning team in close collaboration with bioinformatics experts, biologists, clinicians, as well as other leading academic and industry partners on impactful projects that have the potential to transform patient-care and deliver improved health outcomes.  ","$2,500to$5,000",Monthly,Professional,data engineer
,NTUC ENTERPRISE NEXUS CO-OPERATIVE LIMITED,Full Time,"Engineering, Information Technology",Data Engineer,"Requirements  Preferred qualification and skills:  ·       Advanced degree in computer science, computer engineering, or other technical fields. ·       6-10 years’ experience having developed data engineering capabilities for large and complex franchises. ·       Strong data modeling, schema design and SQL development skills  ·       ETL/ELT implementation and data integration ·       Modern open source data visualization tools, eg. D3js, superset, plotly, leaflet,etc.  ·       Big data platform development (Hadoop/Hive/Hbase/Spark, etc.) ·       REST/Web API development and management ·       Hands-on experience in any modern programming language (Python or Java preferred) ·       Design pattern, 12-factor app principle and modern cloud architecture ·       Self-motivated and proactive, willing to learn new things ·       Good communication skills and strong team player          ","Roles & ResponsibilitiesNTUC Enterprise Co-operative Limited is the holding entity and single largest shareholder of the NTUC group of Social Enterprises. We aim to create a greater social force to do good by harnessing the capabilities of the social enterprises to meet pressing social needs in areas like health and eldercare, childcare, daily essentials, cooked food and financial services. Serving over two million customers, NTUC Enterprise wants to enable and empower all in Singapore to live better and more meaningful lives. The NTUC Enterprise Centre of Excellence for Data, Digitalisation and Technology leads the transformation of the NTUC Social Enterprises by leveraging digital technologies to become more nimble, adaptable and innovative in today’s digital age. he NTUC Enterprise Centre of Excellence for Data, Digitalisation and Technology has been registered as NTUC Enterprise Nexus, a wholly owned subsidiary of NTUC Enterprise. The rapid adoption of technology and mobile devices have contributed to vast new flows of information which are larger in volume, faster in velocity, diverse in variety, and requires veracity of the information for use. This new type of information composed of structured and unstructured data, broadly known as big data (and combined with tools and platforms), if utilized well, could radically improve business performance.  As the organization embarks to become a data-driven organization, significant decisions and value generation will be based on the data that we capture and deploy. The 7 SE’s range in a broad scope of data from FairPrice (retail), Income (insurance), Unity (healthcare), FoodFare (F&B), LearningHub (training), First Campus(ECE), Link (membership). The leaders of these groups are keen to utilize the data to drive growth, deliver customer service, and create personalized experiences. The Data Architecture & Information Management Team will manage and govern the overall datasets of the organization and drive the execution of how the data will be collected, stored, processed and applied across these social enterprises (SEs). In this role you will work with various industries and most diverse datasets in Singapore.    Responsibilities: ·       As a data engineer, you will be creating, writing and maintaining data transfer process and protocols for the data platform.  ·       Work closely with each SE tech heads and their external vendors in mapping out data fields and data transfer process.  ·       Design, build, support and optimize new and existing data models and ETL processes. ·       Develop and support the data pipeline to integrate new data from various data sources with emerging data technologies. ·       Develop and manage the various dashboards for management decision and data visualizations. ·       Define and manage SLA for all data processes and own data quality issues.  ","$5,000to$10,000",Monthly,Professional,data engineer
,NTUC LINK PRIVATE LIMITED,Full Time,"Engineering, Information Technology",Data Engineer,"RequirementsPreferred qualification and skills:  ·       Advanced degree in computer science, computer engineering, or other technical fields. ·       6-10 years’ experience having developed data engineering capabilities for large and complex franchises. ·       Strong data modeling, schema design and SQL development skills  ·       ETL/ELT implementation and data integration ·       Modern open source data visualization tools, eg. D3js, superset, plotly, leaflet,etc.  ·       Big data platform development (Hadoop/Hive/Hbase/Spark, etc.) ·       REST/Web API development and management ·       Hands-on experience in any modern programming language (Python or Java preferred) ·       Design pattern, 12-factor app principle and modern cloud architecture ·       Self-motivated and proactive, willing to learn new things ·       Good communication skills and strong team player","Roles & ResponsibilitiesThe rapid adoption of technology and mobile devices have contributed to vast new flows of information which are larger in volume, faster in velocity, diverse in variety, and requires veracity of the information for use. This new type of information composed of structured and unstructured data, broadly known as big data (and combined with tools and platforms), if utilized well, could radically improve business performance.    As the organization embarks to become a data-driven organization, significant decisions and value generation will be based on the data that we capture and deploy. The 7 SE’s range in a broad scope of data from FairPrice (retail), Income (insurance), Unity (healthcare), FoodFare (F&B), LearningHub (training), First Campus(ECE), Link (membership). The leaders of these groups are keen to utilize the data to drive growth, deliver customer service, and create personalized experiences. The Data Architecture & Information Management Team will manage and govern the overall datasets of the organization and drive the execution of how the data will be collected, stored, processed and applied across these social enterprises (SEs). In this role you will work with various industries and most diverse datasets in Singapore.    Responsibility: ·       As a data engineer, you will be creating, writing and maintaining data transfer process and protocols for the data platform.  ·       Work closely with each SE tech heads and their external vendors in mapping out data fields and data transfer process.  ·       Design, build, support and optimize new and existing data models and ETL processes. ·       Develop and support the data pipeline to integrate new data from various data sources with emerging data technologies. ·       Develop and manage the various dashboards for management decision and data visualizations. ·       Define and manage SLA for all data processes and own data quality issues.  ","$5,000to$10,000",Monthly,Professional,data engineer
"TECHLINK, 31 KAKI BUKIT ROAD 3 417818",JEWEL PAYMENTECH PTE. LTD.,"Permanent, Full Time",Information Technology,Lead Data Engineer,"RequirementsRequired Qualifications:  10+ years of experience building highly scalable, low latency, fault tolerant systems. B.Sc., Masters, or equivalent experience in a quantitative field (Computer Science, Mathematics, Engineering, Artificial Intelligence, etc.). Hands on knowledge of at least 2 programming languages of Python/Java/Scala. In depth knowledge of at least 2 of Hadoop/Spark/Storm/Flink/Kafka. In depth knowledge of at least two NoSQL database (HBase/Cassandra/DynamoDB/Neo4j/Mongo/MemcacheDB) Good knowledge of at least some machine learning algorithms like logistic regression/ SVM/ Random Forests.  Knowledge of advanced data structures and algorithms. Preferred Qualifications: Knowledge of large scale ML systems like Tensorflow/pytorch. Knowledge of advanced machine learning algorithms like CNN/RNN Knowledge of Cloud environments like AWS/GCP. Knowledge of indexing systems like Elastic search/Solr/Lucene.  Proficient in using CI/CD and knowledge of Jenkins/SonarQube/Ansible. You’re a perfect fit us if you are   A master problem solver, and able to use own initiative to develop suitable solutions.  A strong communicator with the ability to convey information to others in a simple and unambiguous way.  An innovative, original thinker approach to job responsibilities, methods and processes.  An energetic person who can be trusted to get a job done.    ","Roles & ResponsibilitiesAs a Data Engineer you will get to work on a wide range of problems using the cutting-edge technologies in Big Data and Data Science. You are required to translate Data Science and Machine learning based solutions into scalable code, and to develop innovative solutions to collect/cleanse/store/process data. In case you are very passionate about building high throughput, low latency, fault tolerant software then this position is for you.  To be successful in this role, you will need to:  Capture/Analyse requirements and lead the design/architecture of solutions to meet requirements.  Write code by using best software development practices/security standards. Lead projects end-to-end from conceptualisation to deployment. Write clear & concise documentation for solutions/code. Contribute ideas within team to build better code.  Continuously improve knowledge on new technologies. Excellent in English, both written and spoken.","$8,000to$10,000",Monthly,Professional,data engineer
"COMCENTRE, 31 EXETER ROAD 239732",HOOQ DIGITAL PTE. LTD.,Permanent,"Engineering, Information Technology",Data Engineer,"RequirementsDesired Skills and Experience  Minimum 3 years of solid development experience within a data warehouse/information management team with strong understanding of programming languages like Java, Python, JavaScript and SQL.   Hands on experience in full-stack development, design and architecture. Experience in creating a REST API that can handle a production load (code + deploy).   Familiarity with AWS (DynamoDB, Redshift, S3, EC2, RDS, Lambda) (Will be an advantage but not mandatory)   Minimum 3 years development experience with ETL/ELT tools (preferably Pentaho DI, Informatica, Datastage or Talend)   Proven experience with Data Warehousing and Big Data technologies   Working knowledge of big Data Technologies like Hadoop, Hive, Spark and streaming/messaging services like Kafka,Spark streaming.   Solid understanding of some BI tools such as Cognos, QlikView or Tableau.   Comfortable working in dynamic fast paced environment with competing priorities. Self-starter and willing and able to learn on your own   Work well within a team environment and willing to accommodate task and duties that maybe outside of your JD for limited time periods ","Roles & ResponsibilitiesWe are looking for a Data Engineer to join our rapidly expanding Data & Analytics team. You will help shape how we build and grow our service in the region. We look for self-starters who demand the best. Key Responsibilities:   Develop ETL/ELT jobs to integrate new data into the data warehouse or build new reporting schemas   Develop data pipelines, both bath and realtime from various platforms into the data lake   Manage various data platforms and seek out new technologies to improve efficiency   Develop advanced analytical models that help the business identify trends within customer base and behavior   Work closely with the business to understand data needs and create data sets to enable reporting and dashboards to monitor business performance   Ensure the data warehouse load jobs run as per schedule and data availability to the business is uninterrupted   Continuously improve the information management platforms of the company to leverage benefits ",,,Manager,data engineer
"OXLEY BIZHUB, 61 UBI ROAD 1 408727",SENSORFLOW PTE. LTD.,Permanent,Information Technology,Data Engineer,"RequirementsSkill Requirements   Solid Computer Science fundamentals, excellent problem-solving skills and a strong understanding of distributed computing principles.   At least 2 years of experience in a similar role, with a proven track record of building scalable and performant data infrastructure.   Expert SQL knowledge and deep experience working with relational and NoSQL databases (e.g. HBase, Cassandra).   Advanced knowledge of Apache Kafka and demonstrated proficiency in Hadoop v2, HDFS, MapReduce.   Experience with stream-processing systems (e.g. Storm, Spark Streaming), big data querying tools (e.g. Pig, Hive) and data serialization frameworks (e.g. Protobuf, Thrift, Avro).   Tech Stack  Data storage: Amazon DynamoDB Service Layer: Amazon Lambda, Amazon API Gateway Service backend: JavaScript/TypeScript, Node.js Web frontend: Angular Mobile: Ionic  For Interested candidates, please email us your full resume to: jobs@sensorflow.org.  We regret only shortlisted candidate will be notified.","Roles & ResponsibilitiesAt SensorFlow we are planning for considerable growth over the next 12 months and need a data engineer to design and develop SensorFlow’s data infrastructure. As we are looking to build the data pipeline from scratch, you will have full autonomy and the technical backing from our engineering team in designing, developing and maintaining this infrastructure. Job Roles & Responsibilities   Design, develop and maintain SensorFlow’s infrastructure for streaming, processing and storage of data. Build tools for effective maintenance and monitoring of the data infrastructure.   Contribute to key data pipeline architecture decisions and lead the implementation of major initiatives.   Work closely with stakeholders to develop scalable and performant solutions for their data requirements, including extraction, transformation and loading of data from a range of data sources.   Develop the team’s data capabilities – share knowledge, enforce best practices and encourage data-driven decisions.  ","$6,000to$8,000",Monthly,Senior Executive,data engineer
"CONNEXIS, 1 FUSIONOPOLIS WAY 138632",GUMI ASIA PTE. LTD.,Full Time,Information Technology,Data Engineer,"Requirements Min. 5 years working in a large analytical data ecosystem Strong technical understanding of data modelling, design, architecture principles, and techniques to take business requirements from concept to implementation Strong knowledge of relational databases and SQL. Knowledge of Python, PHP, Java, Linux architecture and scripting Extensive background extracting and transforming complex data sets. i.e. ETL  Experience with database design and star schema data warehouse theory ","Roles & Responsibilities Design, develop, implement, and evolve data pipelines powering core data sets and key business and performance metrics Identify, troubleshoot, and resolve any performance, system or data related issues, and work to ensure data consistency and integrity Work with Product and Marketing teams on data requirements.  Work with various game teams on data set and data flow to ensure that data requirements are met. Ensure the quality, accuracy, and timeliness of analytical data ","$5,000to$7,000",Monthly,"Professional, Senior Executive",data engineer
"ROBINSON 77, 77 ROBINSON ROAD 068896",TRAVELOKA SERVICES PTE. LTD.,Permanent,Information Technology,Senior Level Data Engineer,"Requirements Passion in big data, software engineering, and systems. Excellent analysis and reasoning of system behaviors 8+ years hands-on experience Uphold best practices and principle around clean code, testing, continuous integration Strong team player and collaborator Having high level of responsibility and resilience in dealing with issues Preferably familiar with big data infrastructure (such as Kafka, Spark, etc.), cloud based infrastructure, varying databases, security concerns would be an advantage. Familiar with Java/JVM. Python is added advantage  Experience working with data infrastructure will be valuable, but your desire to learn data infrastructure is more important. Having systems operational experience is a bonus, but not required. 	  ","Roles & ResponsibilitiesData Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it.   ","$8,300to$15,000",Monthly,Senior Executive,data engineer
"ROBINSON 77, 77 ROBINSON ROAD 068896",TRAVELOKA SERVICES PTE. LTD.,Permanent,Information Technology,Mid -  Senior Level Data Engineer,"Requirements Passion in big data, software engineering, and systems. Excellent analysis and reasoning of system behaviors 6+ years hands-on experience Uphold best practices and principle around clean code, testing, continuous integration Strong team player and collaborator Having high level of responsibility and resilience in dealing with issues Preferably familiar with big data infrastructure (such as Kafka, Spark, etc.), cloud based infrastructure, varying databases, security concerns would be an advantage. Familiar with Java/JVM. Python is added advantage  Experience working with data infrastructure will be valuable, but your desire to learn data infrastructure is more important. Having systems operational experience is a bonus, but not required. 	  ","Roles & ResponsibilitiesData Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it.   ","$6,100to$10,700",Monthly,Executive,data engineer
"ROBINSON 77, 77 ROBINSON ROAD 068896",TRAVELOKA SERVICES PTE. LTD.,Permanent,Information Technology,Mid Level Data Engineer,"Requirements Passion in big data, software engineering, and systems. Excellent analysis and reasoning of system behaviors 3+ years hands-on experience Uphold best practices and principle around clean code, testing, continuous integration Strong team player and collaborator Having high level of responsibility and resilience in dealing with issues Preferably familiar with big data infrastructure (such as Kafka, Spark, etc.), cloud based infrastructure, varying databases, security concerns would be an advantage. Familiar with Java/JVM. Python is added advantage  Experience working with data infrastructure will be valuable, but your desire to learn data infrastructure is more important. Having systems operational experience is a bonus, but not required. 	  ","Roles & ResponsibilitiesData Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it. 	  ","$4,300to$7,600",Monthly,Executive,data engineer
"ROBINSON 77, 77 ROBINSON ROAD 068896",TRAVELOKA SERVICES PTE. LTD.,Permanent,Information Technology,Junior - Mid Level Data Engineer,"Requirements Passion in big data, software engineering, and systems. Excellent analysis and reasoning of system behaviors Fresh Graduate to 4 years hands-on experience Uphold best practices and principle around clean code, testing, continuous integration Strong team player and collaborator Having high level of responsibility and resilience in dealing with issues Preferably familiar with big data infrastructure (such as Kafka, Spark, etc.), cloud based infrastructure, varying databases, security concerns would be an advantage. Familiar with Java/JVM. Python is added advantage  Experience working with data infrastructure will be valuable, but your desire to learn data infrastructure is more important. Having systems operational experience is a bonus, but not required. 	  ","Roles & ResponsibilitiesData Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it. 	  ","$3,000to$6,000",Monthly,Fresh/entry level,data engineer
,JEWEL PAYMENTECH PTE. LTD.,"Permanent, Full Time",Information Technology,Data Engineer,"Requirements B.Sc., Masters, or equivalent experience in a quantitative field (Computer Science, Mathematics, Engineering, Artificial Intelligence, etc.). Knowledge in the use and application of Python to develop complex software. General machine learning techniques and technologies (e.g., Bayesian classifiers, regression techniques, graphical models, working with unbalanced data-sets) as well as applications (e.g., predictive analytics). NoSQL Database Programming/Development. Manipulation of various types of data; data cleaning, filtering, and pre-processing for example with text/images. Knowledge and experience in the use of cloud computing platforms (AWS/Azure/GCP/etc). SQL familiarity and database technologies (e.g., row versus column stores, in-memory DB, DB clustering, HA for DB). Familiarity and experience with Linux environments. Understanding batch (e.g., Apache Hadoop / Map Reduce) and stream processing approaches / frameworks (e.g., Apache Spark).    You’re a perfect fit us if you are  A master problem solver, and able to use own initiative to develop suitable solutions. A strong communicator with the ability to convey information to others in a simple and unambiguous way. An innovative, original thinker approach to job responsibilities, methods and processes. An energetic person who can be trusted to get a job done. ","Roles & ResponsibilitiesAs a Data Engineer you will get to work on a wide range of problems using the cutting-edge technologies in Big Data and Data Science. You are required to translate Data Science and Machine learning based solutions into scalable code, and to develop innovative solutions to collect/cleanse/store/process data. In case you are very passionate about building high throughput, low latency, fault tolerant software then this position is for you. To be successful in this role, you will need to:  Analyze requirements and deliver solutions that meet requirements. Write code by using best software development practices. Produce code that meets security standards. Estimate timelines and deliver solutions within agreed timeline. Write clear & concise documentation for solutions/code. Contribute ideas within team to build better code. Continuously improve knowledge on new technologies. Excellent in English, both written and spoken. ","$4,000to$5,000",Monthly,Professional,data engineer
"AZ @ PAYA LEBAR, 140 PAYA LEBAR ROAD 409015",THE SUPREME HR ADVISORY PTE. LTD.,Full Time,Information Technology,L1 Desktop Engineer - Scotts Road - OS Backup & Server’s Data Backup (A1),"RequirementsMAIN ROLES AND RESPONSIBILITIES • OS Backup & Server’s Data Backup • Network, VOIP & Server Administration • PC’s Installation & Configuration • OS & Software Installation & Configuration • Troubleshooting PC’s, Notebook’s, Printer’s & Network Equipment • Troubleshooting OS & Software issues • Create/ Manage computer & notebook Asset List",Roles & Responsibilitieslocation: Scotts Road   ,"$1,800to$2,100",Monthly,Senior Executive,data engineer
12A UPPER CIRCULAR ROAD 058410,EYEOTA PTE. LTD.,Full Time,Information Technology,"Data Engineer, Modeling & Onboarding","RequirementsYou:  Minimum 4 years Experience working with petabyte-scale data Experience architecting, developing, and operating data warehouses, big data analytics platforms, and high-velocity data pipelines Exposure to modern Big Data tech: Cassandra/Scylla, Kafka, Ceph, the Hadoop Stack, Spark, Flume, Hive, Druid etc… while at the same time understanding that certain problems may require completely novel solutions Exposure to one or more modern ML tech stacks: Spark ML-Lib, Tensorflow, Keras, GCP ML Stack, AWS Sagemaker Deep technical understanding of Golang and/or Java Production experience with Python is a plus Exposure to configuration management tools such as Ansible or Salt Exposure to IAAS platforms such as AWS, GCP, Azure… Strong buyer of Agile/Lean values Experience with supporting and troubleshooting large systems ","Roles & ResponsibilitiesEyeota is looking for an exceptional Data Engineer who can contribute to building a world-class big data engineering stack that will be used to fuel our Machine Learning product pipeline. This person will be contributing to the architecture, operation, and enhancement of:   Our petabyte-scale data platform with a key focus on finding solutions that can support the Machine Learning product roadmap. This platform ingests terabytes of data daily which need to be made available to a variety of Machine Learning use cases. Our bespoke Machine Learning pipelines. This will also provide opportunities to contribute to the prototyping, building, and deployment of Machine Learning models.  The candidate should have significant experience in developing and operating a modern data pipeline platform and should have a keen interest in Machine Learning and Data Science","$8,000to$15,000",Monthly,Middle Management,data engineer
"SHAW TOWERS, 100 BEACH ROAD 189702",PEOPLE PROFILERS PTE. LTD.,Permanent,Information Technology,Data Engineer (Python / Central),"RequirementsRequirements  Degree in a technical/quantitative discipline e.g. Mathematics/Computer Science Ideally at least 1 year of relevant experience Experience in Python programming, web crawling Knowledge of AWS platforms     Helpful:  -Familiarity with ETL design & database management -Familiarity with NoSQL database -Knowledge of machine learning techniques   Successful candidates can expect a very competitive salary package with comprehensive benefits. Interested applicants may wish to email your resume in a detailed Word format to ruth.gan@peopleprofilers.com. Please include last drawn and expected salaries and notice period. We regret that only shortlisted candidates will be notified.   Gan Huiru  Recruitment Consultant  Tel: +65 6594 9897 Fax +65 6835 7890   Address: 100 Beach Road #33-06 Shaw Tower Singapore 189702 Email: ruth.gan@peopleprofilers.com EA License Number: 02C4944 Registration Number: R1768917","Roles & Responsibilities Join a fast-expanding and highly-specialised tech company Good exposure in data analytics projects Central location Great company culture & working environment    Requirements  Make enhancements to data collection, structure & delivery Use automated methods for data compilation Build robust batch & streaming pipelines Write clear and high-quality code (including in Python) Work well in a team ","$4,000to$6,000",Monthly,Junior Executive,data engineer
"INFINEON, 8 KALLANG SECTOR 349282",INFINEON TECHNOLOGIES ASIA PACIFIC PTE LTD,"Permanent, Full Time","Engineering, Information Technology, Manufacturing",Senior Engineer  /  Engineer - Data Management,"RequirementsYou are best equipped for this task if you have:  Bachelor's Degree in Engineering / Information Technology Good knowledge on computer hardware, and standard application support Good communication & analytical problem-solving skills Previous experience in the semiconductor testing field will be advantageous Candidate must be comfortable working on both Windows and Linux environment Good Understanding of Programming / Query languages / Visualisation software such as Python,Perl, Javascript, C / C++, SQL,Tableau  Please apply via https://www.infineon.com/cms/en/careers/jobsearch/jobsearch/32237-Senior-Engineer---Engineer-Data-Management/ ","Roles & ResponsibilitiesYou are responsible to support the automated / productivity projects which are relevant to Automated Test Equipment (ATE), and improve data handling towards automation. In your new role you will:  Support automated / productivity projects relevant with Automated Test Equipment (ATE) & data handling Improve data clarity with scripting towards yield improvement Drive & improve data relevant KPI (data quality, process time) towards data automation Responsible for production of automated computer hardware and software for test equipment Install and configure, investigate, diagnose and solve ATE computer software and hardware issues ","$3,000to$6,000",Monthly,Executive,data engineer
"THE SYNERGY, 1 INTERNATIONAL BUSINESS PARK 609917",PM ASIA PROJECT SERVICES PTE. LTD.,"Permanent, Contract",Engineering,Lead Electrical Engineer (Data Centre),"Requirements Degree in Electrical Engineering, Chartered Engineer preferred At least 10 years’ of experience of relevant experience  Experience within the Data Centre/ Pharmaceutical / Food & Nutriton industry preferred ","Roles & ResponsibilitiesOverview:  Lead the Electrical Design of large scale, high end industrial facilities.  Have knowledge and experience in designing Electrical systems for Data Centres, including Medium Voltage, Low Voltage Power Systems, Emergency Power Systems, ELV’s (CCTV, Interlocks, Fire Alarm, Power Monitoring System) Work in a multi-disciplinary design office environment for global clients. Display a personal commitment to safety, hold safety as a core value and provide safety leadership in the performance of all work activities Be quality focussed, producing well engineered designs to the highest standards in an ISO9000 Quality System environment   ","$7,500to$11,000",Monthly,Senior Executive,data engineer
"THE SIEMENS CENTRE, 60 MACPHERSON ROAD 348615",SIEMENS PTE. LTD.,"Permanent, Full Time",Information Technology,Big Data Engineer (287102),"RequirementsWhat do I need to qualify for this job?  University degree in an appropriate area (e.g.informatics) At least 2 years of relevant work experience Experience with modern big data technologies like Hadoop, MapReduce, Kafka, Hive, Presto, Spark, etc. Experience with cloud solutions like AWS Experience with programming languages like SQL, Scala, Python, Java Experience with enterprise application integration ","Roles & ResponsibilitiesWhat are my responsibilities?  Responsible for the integration of large, structured and unstructured data volumes into the existing cloud platforms Development of scalable end-to-end data pipelines for batch and stream processing Execution of the data integration activities (ETL /ELT) for populating the data lake and integrating diverse data sources Execution an further development of the physical implementation of the logical data model into a physical implementation in the data lake Implementation of solutions for reference data and master data management within the context of the mobility data business Execution of data quality measurements and implementation of data quality improvement ","$4,000to$7,000",Monthly,"Professional, Senior Executive",data engineer
"PARKVIEW SQUARE, 600 NORTH BRIDGE ROAD 188778",NIOMETRICS (PTE.) LTD.,Permanent,Information Technology,High-Performance Data Engineer,RequirementsWHAT WE VALUE  Bachelor’s or Higher Degree in Computer Science or equivalent Software craftsmanship Attention to reliability and successful delivery Experience with large C code bases and high-performance C programming Familiarity with shared memory data structures and parallel algorithms Proficiency with Linux system & development tools ,"Roles & ResponsibilitiesWHAT WE DO We invite you to be part of our ambitious, close-knit team creating systems for large customers who need to crunch through Tbps of data in real-time. Our approach is relentless performance-oriented software engineering vs. server sprawl in our customers' datacentres. You will use the latest high-end hardware and continuously devise ways to push the envelope of software performance. We build in-house systems if we must. We had to for indexing 1M 60-column rows/s, for aggregating high throughput event streams over hundreds of combinations of dimensions, and for pattern matching 5M patterns at 100Gb/s per 2RU. We use these to solve real customer problems. You will experiment wildly. For example we implemented network monitoring using a GPU, and we tested 4-socket machines with 2T RAM. Our current favourite platform is a 2-socket system with E5-2699v4 CPUs (88 lcores in total), 4x40Gbps NICs and 1T RAM, which we use to process 160Gbps. You will help us build a successful software platform for the long run. We invest a lot in flexibility, such as with our extensible rule engine and declarative aggregation system that empowers our analysts and helps us minimise the C code we have to write for supporting disparate use-cases. We know the devil is in the details. You will improve performance through better memory allocation systems and better data structures, all while ensuring that they are integrated with Address Sanitizer and fully tested using unit tests and end-to-end regression tests. We work end-to-end. You will implement data engineering solutions that are both efficient and secure for handling events from 500 million users, and to extract insight without leaking individual information. We want to show off. To attract the best programmers we plan to showcase our technology. You can be part of our effort to open-source interesting pieces of our technology stack.   YOUR ROLE AS HIGH-PERFORMANCE DATA ENGINEER As a High-Performance Data Engineer, you will create and maintain tools, mainly in C, for crunching large amounts of data in files or streams. You will have to think both big, in terms of overall architecture, and small, in terms of low-level optimisations, to deliver solutions that are reusable, and match the performance of the best hardware. Every capability you add directly translates to new offerings made possible. Every percent of performance improvement directly translates to large cost savings. At the same time, the correctness and reliability of your work will be the cornerstone to our customers’ trust.","$5,500to$11,000",Monthly,Professional,data engineer
"ASIA SQUARE TOWER 1, 8 MARINA VIEW 018960",CAPITA PTE. LTD.,"Contract, Full Time",Information Technology,Data Center (System) Engineer,"RequirementsRequirements:  Minimum 1 year of experience in Data Center Experience in monitoring of servers and network Tape management, facility checks  Interested candidates, please click the Äpply Now"" below Only shortlisted applicants will be notified by our consultants.   CAPITA PTE LTD | EA License No : 08C2893 Tan Chin Yin | REG No : R1762272","Roles & Responsibilities​Responsibility   Monitoring of ESX, OS Batch Monitoring Service Request Execution Coordinate with Appointed vendor for Offsite tape archival Restoration of VM/s Batch execution and monitoring, batch output & event handling, batch incident identification, escalation & reporting Tape backups, media handling, tape library operations and off-site storage Server health checks Start-up, shutdown, reboot/restart systems & services Generate daily/ weekly/ monthly/ yearly/ ad-hoc reports and dispatch reports to users & customers Adhere to all operational & physical security procedures Provide operational support during DR Exercises at DR sites Work towards acceptable audit rating ","$2,500to$3,000",Monthly,Junior Executive,data engineer
,Company Undisclosed,Full Time,Banking and Finance,Data Development Engineer,"RequirementsWhat You’ll Bring:    Background in Computer Science, Engineering, Math or Physics, with minimum Bachelor’s degree. Proof of good academic record (such as GPA and other relevant test scores) Effective problem-solving skills both independently and as member of a team Good communication skills: must be fluent in English, spoken and written Experience working under Linux environment, familiar with vi or emacs for editing files Interested in applying technology to real world situation, comfortable working in fast paced work environment, detail oriented and capable performing tasks under time pressure Experience with programming in C/C++, familiar with common algorithms and data structures (binary tree, sorting, etc), Object Oriented programming and design patterns. Familiarity with compilers, debuggers under Linux (gcc, g++, gdb). Experience with scripting languages, such as Perl, Python, and shell scripting Knowledge of basic statistics/probability, familiar with concepts such as correlation, standard deviation and how to compute Familiarity with databases (such as MySQL) ","Roles & ResponsibilitiesWorldQuant develops and deploys systematic financial strategies across a variety of asset classes and global markets. We seek to produce high-quality predictive signals (Alphas) through our proprietary research platform to employ financial strategies focused on exploiting market inefficiencies. Our teams work collaboratively to drive the production of Alphas and financial strategies – the foundation of a sustainable, global investment platform.   Technologists at WorldQuant research, design, code, test and deploy projects while working collaboratively with researchers and portfolio managers. Our environment is relaxed yet intellectually intense. Our teams are lean and agile, which means rapid prototyping of products with immediate user feedback. We seek people who think in code, aspire to solve undiscovered computer science challenges and are motivated by being around like-minded people. In fact, of the 600 employees globally, approximately 500 of them code on a daily basis.   WorldQuant’s success is built on a culture that pairs academic sensibility with accountability for results. Employees are encouraged to think openly about problems, balancing intellectualism and practicality. Great ideas come from anyone, anywhere. Employees are encouraged to challenge conventional thinking and possess a mindset of continuous improvement. That’s a key ingredient in remaining a leader in any industry.      Our goal is to hire the best and the brightest. We value intellectual horsepower first and foremost, and people who demonstrate an exceptional talent. There is no roadmap to future success, so we need people who can help us create it. Our collective intelligence will drive us there.   The Role: In this role, candidate will implement and maintain software towards creation of new datasets. Data sets will be consumed internally by researchers and utilized by internal quantitative models. Candidate will design efficient algorithms for collection, analysis, processing and filtering of data.    Work with the global team in designing and implementing data retrieval software for various data sets Implement the rules and procedures that ensure integrity in data sets Collect and analyze statistics on market data applications and devise approaches to improve the relevant processes Develop and enhance monitoring tools to detect various types of errors in data ","$6,000to$8,000",Monthly,Professional,data engineer
,Company Undisclosed,"Permanent, Full Time","Engineering, Manufacturing",MCT Big Data Engineer,"RequirementsQualifications and Experience: * B.S degree or M.S. degree with 2 years’ experience in Computer Engineering, Industrial Engineering, or any other discipline with extensive programming or machine learning work  * Minimum 2 years of experience working in big data and data science projects and teams * Extensive experience with Java, Scala, Python in Hadoop ecosystem (Spark, Hive, HBase, etc.) is a must * Extensive experience with at least one relational databases (MS SQL, Oracle, MySQL, Teradata, etc.) is a must * Experience with building analytical web applications and data visualization technologies (Django, Javascript, Bootstrap, D3, etc.) is a plus * Good grasp of statistical and scientific programming packages in Python, R, etc. * Good grasp of data science concepts with emphasis on machine learning techniques is a plus * Experience with image processing (OpenCV, Python PIL, scikit-image, etc.) is a plus * Proficiency with collaborative source code management and documentation tools. (GIT, JIRA, Confluence, etc.) * Strong communication skills (written, verbal and presentation) * Willing to do international travel","Roles & ResponsibilitiesDo you have a broad theoretical and practical understanding of data engineering and data science? Can you wrangle large scale multidimensional data effectively? Are you always curious to learn something new? Do you love to solve engineering puzzles and optimize complex systems? Can you translate an idea in to an algorithm and make it into a product with quality and scalability in mind? Are you looking for window to the world ?   If so, you may be a great candidate for an Manufacturing Central Team (MCT) Data Engineer position at company, a global, Fortune 500 leader in the semiconductor industry. This position will be based in Singapore.   As an MCT Data Engineer at company, you will:   * Work with an international team of data scientists, data engineers, software engineers, process and equipment engineers, process integration engineers, yield enhancement engineers, R&D, etc. in a collaborative manner to develop new data science solutions that improve quality, improve yield, reduce deviations, improve manufacturing cycle time, reduce cost, extend manufacturing capabilities, etc. * Draw from a broad background of data-mining techniques in mathematics, statistics, information technology, machine learning, data engineering, design of experiments (DOE), visualization, etc. to discover insightful patterns in semiconductor manufacturing data * Work on projects and develop solutions that would be of high impact to various areas at all manufacturing fabs * Deliver polished presentations of data acquisition, data flow, data preparation and data presentation layer to internal customers and leaders to inform business strategy, streamline operations, and execute to revenue goals * In short, be a full-stack data engineer who can take an idea, access and prepare necessary data, work with data scientists to create machine learning models, develop it to an application with intuitive user interface, integrate with any pre-existing systems, demonstrate successful use cases and wins, etc.   Responsibilities and Tasks include, but not limited to:   * Understanding business needs and strategy to develop data science solutions * Collaborating with other data engineers and business process experts to access existing data in data warehouse and big data environments * Developing new or enhancing prior data acquisition and ETL pipelines from various sources into big data ecosystem. * Preparing data for machine learning using appropriate steps and methods, which may include data cleaning, transformation, augmentation, enrichment, sampling, etc. * Working with various scientific data such as equipment sensor data and logs, image and various types of signals, manufacturing process data, etc. to extract meaningful information for analytics * Creating intuitive user interface for interactive data visualization to explain insights from data * Preparing and delivering powerful presentations with rich data visualizations and meaningful business conclusions * Documenting the train of thoughts used to design and implement solutions along with managed source code * Traveling and participating in various internal forums for strategy building and to build solutions in collaboration with various manufacturing sites","$3,400to$6,800",Monthly,Non-executive,data engineer
,Company Undisclosed,"Permanent, Full Time","Engineering, Manufacturing",IT - BIG DATA ENGINEER,"RequirementsRequirements: Bachelor’s or Master’s degree Computer Science, Electrical & Electronics/Computer/Software Engineering, Information Systems or related fields.   Fresh graduates are welcome to apply and for those with good understanding and hands-on experience in the following areas will be advantageous.    Hadoop based technologies such as HDFS, MapReduce, Hive, MongoDB, HBase, Spark etc.  Data warehousing solutions and latest (NoSQL) database technologies.  Programming or scripting languages like Java, Linux, Matlab, C#/C++, Python, Perl and/or R on Linux/Windows platforms.  Big Data visualization and reporting software like Tableau.  ETL/BI solutions using Microsoft SSIS, Informatica or having DB programming experience (TSQL, PLSQL).    Effective oral and written communication with strong analytical, problem solving, multitasking and project management skills are essential on the job.","Roles & ResponsibilitiesResponsibilities: Be part of a DevOps team that design, build and maintain innovative Smart Manufacturing solutions and Big Data platform.  Participate in Agile development lifecycle for software & solution related to Smart Manufacturing and Big Data platform.   Work with Data Science within company to develop, automate and maintain reliable data analytic and mining solutions for Smart Manufacturing and Big Data platform.  Ability to assess current IT environments and make recommendations to increase capacity needs.  Communicate, collaborate and coordinate on Smart Manufacturing and Big Data related activities to various level of stakeholders and senior management.","$3,400to$6,800",Monthly,Non-executive,data engineer
,Company Undisclosed,"Permanent, Full Time","Engineering, Manufacturing",MCT Big Data Senior Engineer,"RequirementsQualifications and Experience: * B.S degree or M.S. degree with 2 years’ experience in Computer Engineering, Industrial Engineering, or any other discipline with extensive programming or machine learning work  * Minimum 2 years of experience working in big data and data science projects and teams * Extensive experience with Java, Scala, Python in Hadoop ecosystem (Spark, Hive, HBase, etc.) is a must * Extensive experience with at least one relational databases (MS SQL, Oracle, MySQL, Teradata, etc.) is a must * Experience with building analytical web applications and data visualization technologies (Django, Javascript, Bootstrap, D3, etc.) is a plus * Good grasp of statistical and scientific programming packages in Python, R, etc. * Good grasp of data science concepts with emphasis on machine learning techniques is a plus * Experience with image processing (OpenCV, Python PIL, scikit-image, etc.) is a plus * Proficiency with collaborative source code management and documentation tools. (GIT, JIRA, Confluence, etc.) * Strong communication skills (written, verbal and presentation) * Willing to do international travel","Roles & ResponsibilitiesDo you have a broad theoretical and practical understanding of data engineering and data science? Can you wrangle large scale multidimensional data effectively? Are you always curious to learn something new? Do you love to solve engineering puzzles and optimize complex systems? Can you translate an idea in to an algorithm and make it into a product with quality and scalability in mind? Are you looking for window to the world ?   If so, you may be a great candidate for an Manufacturing Central Team (MCT) Data Engineer position at company, a global, Fortune 500 leader in the semiconductor industry. This position will be based in Singapore.   As an MCT Data Engineer at company, you will:   * Work with an international team of data scientists, data engineers, software engineers, process and equipment engineers, process integration engineers, yield enhancement engineers, R&D, etc. in a collaborative manner to develop new data science solutions that improve quality, improve yield, reduce deviations, improve manufacturing cycle time, reduce cost, extend manufacturing capabilities, etc. * Draw from a broad background of data-mining techniques in mathematics, statistics, information technology, machine learning, data engineering, design of experiments (DOE), visualization, etc. to discover insightful patterns in semiconductor manufacturing data * Work on projects and develop solutions that would be of high impact to various areas at all manufacturing fabs * Deliver polished presentations of data acquisition, data flow, data preparation and data presentation layer to internal customers and leaders to inform business strategy, streamline operations, and execute to revenue goals * In short, be a full-stack data engineer who can take an idea, access and prepare necessary data, work with data scientists to create machine learning models, develop it to an application with intuitive user interface, integrate with any pre-existing systems, demonstrate successful use cases and wins, etc.   Responsibilities and Tasks include, but not limited to:   * Understanding business needs and strategy to develop data science solutions * Collaborating with other data engineers and business process experts to access existing data in data warehouse and big data environments * Developing new or enhancing prior data acquisition and ETL pipelines from various sources into big data ecosystem. * Preparing data for machine learning using appropriate steps and methods, which may include data cleaning, transformation, augmentation, enrichment, sampling, etc. * Working with various scientific data such as equipment sensor data and logs, image and various types of signals, manufacturing process data, etc. to extract meaningful information for analytics * Creating intuitive user interface for interactive data visualization to explain insights from data * Preparing and delivering powerful presentations with rich data visualizations and meaningful business conclusions * Documenting the train of thoughts used to design and implement solutions along with managed source code * Traveling and participating in various internal forums for strategy building and to build solutions in collaboration with various manufacturing sites","$5,000to$10,000",Monthly,Non-executive,data engineer
,Company Undisclosed,"Permanent, Full Time","Engineering, Manufacturing",MCT QE Engineer (Data Specialist),"RequirementsRequirements: •A Computer Science degree or related disciplines. •A solid and wide-reaching foundation in programming and database structures is required. (SQL, Perl, C++, PHP) •A good knowledge of Tableau or related software that able to present data in a meaningful manner to enable SMEs to perform analysis. •In addition to possessing technical know-how and being communication savvy, data specialists must also be creative problem solvers","Roles & ResponsibilitiesResponsibilities •As a Manufacturing Central Team Quality Engineer at company, you will be a member of the worldwide Quality Team responsible for QDRs, reducing variation and deviations for all company's Fabs. •Specific to your role as Data Specialist, your work will encompass the responsibilities of an architect, a designer and a developer/administrator. You need to have an in-depth understanding of database and structure and use those skills and knowledge to maintain their stability and reliability. •You will work with SMEs to identify their needs and to design and implement reporting dashboard and data structure to meet their requirements for quality improvement; your work will also extend to recommend improvements to meet the demands of swiftly changing interface technology; you will also be in charge of performing backups procedures to protect the data.","$4,000to$8,000",Monthly,Non-executive,data engineer
,KPLER PTE. LTD.,Full Time,Information Technology,Senior Data Engineer,"RequirementsKnowledge & Experience   Must Have    At least 5 years of experience in similar roles working Python and/or Scala and cloud infrastructure (Amazon AWS) on linux servers Experience with SQL (PostgreSQL or equivalent) Proficiency developing automated unit and integration tests and continuous integration  Ability to learn quickly and deliver high quality code in a fast-paced, dynamic team environment     Nice to have    Knowledge of SqlAlchemy, Scala Play2, ElasticSearch, Heroku Experience of data management lifecycles (collection, cataloguing, ETL design) Experience with geographic information systems (PostGIS for instance) Experience of LEAN methodologies and approaches to process optimisation ","Roles & ResponsibilitiesWho are we ?   Kpler is an intelligence company providing transparency solutions in energy markets. We develop proprietary technologies that systematically aggregate data from hundreds of sources ranging from logistical and commercial, to governmental and shipping databases. By connecting the dots across fragmented information landscapes, we are able to provide our clients with unique, real-time market coverage.   We rely on intelligent people to build intelligent software. Our team is composed of individuals of various backgrounds, with diversified skill sets and international experiences. Our clients are players across the energy market spectrum, with offices from Houston to Singapore.   Role Purpose   We are looking for a Senior Data Engineer to join our software engineering team in Singapore to work on our data pipelines (collect, manage, data lake storage), data algorithms (based on either business rules, constraint programming, ML, etc.) and be a technical referent in Python and/or Scala. Our future team member will have a good understanding of data collection and management of complex B2B business rules. Also, you will take ownership of large features from technical design through completion. At Kpler, the Senior Data Engineer is at the centre of our research delivery and will have a key role in defining architecture, helping identify and implement areas for improvement within our data methodologies and technologies used. Based in Singapore, you will also interact with the Paris engineering team; being able to communicate efficiently in English (mandatory, we have more than 15 nationalities at Kpler!) and work with remote team members is key.   Also, you will:  Coach and work on code review with more junior engineers Ensure integrity of data through creative, robust and sustainable quality control methods Participate in operations/support of the real-time platforms Participate in defining coding standards, specifications and development processes Translating technical concepts to/from non-technical language ","$7,000to$10,000",Monthly,Senior Executive,data engineer
,"JPMORGAN CHASE BANK, N.A.","Permanent, Full Time",Information Technology,Global Technology Infrastructure Data Center Operations Engineer I - Analyst,"RequirementsThis role requires a wide variety of strengths and capabilities, including:    Understanding of information technology concepts in a working or academic environment General knowledge of a physical IT infrastructure (server, networking, storage) Some understanding of network concepts (switching, routing, perimeter security) Some understanding of operating systems (Windows, Linux, AIX) A mindset that challenges rather than simply understands and accepts Passionate about technology, innovation and continuous learning Ability to integrate well into a team, connect and collaborate effectively with the wider organization Being obsessed about our customers’ experience Flexibility to work non-business hours that may include weekends and/or holidays Ability to support frequent standing, walking, pushing, pulling, bending, reaching, and lifting up to 50lbs Willingness for occasional travel between sites          Our Global Technology Infrastructure group is a team of innovators rewarded with innovators who love technology as much as you do. Together, you will use a disciplined, innovative and a business focused approach to develop a wide variety of high-quality products and solutions. You will work in a stable, resilient and secure operating environment where you—and the products you deliver—will thrive.   When you work at JPMorgan Chase & Co., you are not just working at a global financial institution. You are an integral part of one of the world’s biggest tech companies. In 14 technology hubs worldwide, our team of 40,000+ technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $9.5B+ annual investment in technology enables us to hire people to create innovative solutions that will not only transform the financial services industry, but also change the world.    At JPMorgan Chase & Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity.  We encourage professional growth and career development, and offer competitive benefits and compensation.  If you are looking to build your career as part of a global technology team, tackling big challenges that impact the lives of people and companies all around the world, we want to meet you.      ","Roles & ResponsibilitiesAs a Data Center Operations Engineer I, your mission is to support the day-to-day technology operations of JPMorgan Chase mission critical data centers. The purpose of this role is to maintain operational stability and handle customer requests while working on shifts and on calls to support the 24x7 operation. You will be responsible for installing and configuring enterprise class technology hardware, troubleshooting hardware and network issues, maintain change control process in the data center, and support 3rd party vendor activities. This position is full-time, working with team members on a rotating basis.  ","$4,000to$8,000",Monthly,Professional,data engineer
"SUNTEC TOWER FIVE, 5 TEMASEK BOULEVARD 038985",ALLEGIS GROUP SINGAPORE PRIVATE LIMITED,Contract,Information Technology,Data Quality Engineer,"RequirementsData quality implementation experience 2) Preferably with Infomatica Data Quality tool OR other DQ tools like Trilium, IBM QualityStage or Microsoft data quality services(DQS) 3) Change and release management experience Experience with data warehousing and data mart development will be a good to have. Skills & Competencies: (free text) Technical skills refer to job-specific knowledge, skills and abilities (e.g. programming, 3D modelling, marketing, etc), Generic skills refer to skills that can be transferred from one job to another (e.g. communication, problem-solving). 1) Data quality implementation experience; 2) Preferably with Infomatica Data Quality tooling experience OR other DQ tools like Trilium, IBM QualityStage or Microsoft data quality services(DQS) 3) Strong change and release management experience Experience with data warehousing and data mart development will be a good to have.","Roles & ResponsibilitiesThis is a new initiative of the bank within a pioneer team to bring data quality into operations. The primary work comprises of ETL tool programming, data quality implementation, data governance and change & release management.  ","$9,000to$12,000",Monthly,Executive,data engineer
"SUNTEC TOWER FIVE, 5 TEMASEK BOULEVARD 038985",ALLEGIS GROUP SINGAPORE PRIVATE LIMITED,Contract,Information Technology,Data Quality Engineer,"RequirementsData quality implementation experience 2) Preferably with Infomatica Data Quality tool OR other DQ tools like Trilium, IBM QualityStage or Microsoft data quality services(DQS) 3) Change and release management experience Experience with data warehousing and data mart development will be a good to have. Skills & Competencies: (free text) Technical skills refer to job-specific knowledge, skills and abilities (e.g. programming, 3D modelling, marketing, etc), Generic skills refer to skills that can be transferred from one job to another (e.g. communication, problem-solving). 1) Data quality implementation experience; 2) Preferably with Infomatica Data Quality tooling experience OR other DQ tools like Trilium, IBM QualityStage or Microsoft data quality services(DQS) 3) Strong change and release management experience Experience with data warehousing and data mart development will be a good to have.","Roles & ResponsibilitiesThis is a new initiative of the bank within a pioneer team to bring data quality into operations. The primary work comprises of ETL tool programming, data quality implementation, data governance and change & release management.  ","$5,000to$9,000",Monthly,Executive,data engineer
,DBS BANK LTD.,"Permanent, Full Time",Banking and Finance,"Senior Associate, DevOps Engineer, Group Consumer Banking and Big Data Analytics Tech (180004DY)","Requirements Experience and exposure to other DevOps related infrastructure software (such as ZABBIX, Puppet, Graphite, ELK stack, Kickstart, tcp wrappers, iptables, yum/apt-get) for classic sys admin functions (building hosts, monitoring, alerting, account management, releasing software, and security. Oracle, or MySQL 5.X, IBM DB2 (Any), SqLite (for Mobile devices), mariadb Build Tool: Ant, Maven, Gradle (Any) Version Control: CVS, SVN, GIT WebSphere Administration WebSphere MQ administration. DevOps – Jenkins/Bamboo, SonarCube, HP Fortify. Atlasian tools: Bitbuket, JIRA, Confluence, Bamboo and SharePoint Mobile application tools – Kony IDE, XCode, Android Studio, Ant, Maven, Gradle  AIX/Unix Administration Tomcat/IBM HTTP Server administration ","Roles & ResponsibilitiesJob Purpose  The role forms part of the Insurance and Investment Platform technology team. Develop and deliver technology solutions relating to web and mobile applications across wealth customer segments.   Key Accountabilities  Engineer CI/CT/CD pipeline that is optimized to run within minutes Enforce best practices in code quality and release/deployment process to achieve near zero production incidents    Responsibilities  Architect, build and maintain continuous integration, testing and deployment (CI/CT/CD) pipeline for web and mobile apps Collaborate with architects, development engineers and system administrators to provision and maintain the platform infrastructure both on premise as well as cloud (for development, test and production environments) Build and maintain system and application health check and house-keeping jobs Troubleshoot system and connectivity errors and follow up with administrators, vendors or other teams for timely resolution Develop, maintain and document best practices in source control management and infrastructure as code  Track, maintain and renew infrastructure, web and mobile application key-stores and profiles Track, enforce and maintain code quality, security and performance reports Identify improvement areas and engage the required stakeholders to successful implement the changes Keep track of evolving technologies and perform proof of concept integrations for successful platform integrations as per roadmap Maintain platform collaboration tools such as JIRA and Confluence ","$5,000to$10,000",Monthly,Senior Executive,data engineer
"MAPLETREE BUSINESS CITY, 10 PASIR PANJANG ROAD 117438",GOVERNMENT TECHNOLOGY AGENCY,Permanent,"Information Technology, Public / Civil Service",Data Engineer,,"Roles & ResponsibilitiesThe Government Digital Services team is seeking an accomplished Data Engineer. We are a team in GovTech that aims to design and develop software applications that help government agencies to better serve the needs of Singaporeans. We adopt an Agile development approach and work towards adopting tech best practices and cutting edge tools.
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
What To Expect:

Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure through multiple data centers.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.

How To Succeed:

Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. Preferably with the experience of using the following software/tools:

Experience with big data tools: Hadoop, Spark, Kafka, RabbitMQ etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with either of these languages: Python, Java.


",,,,data engineer
,DBS BANK LTD.,"Permanent, Full Time",Banking and Finance,"AVP  /  Senior Associate, Data Engineer, IBG Digital, Institutional Banking Group (1800044U)","Requirements Master’s Degree in software Engineering, Computer Science or related fields with minimum 3 years data engineering work experience in big data analytics environment Strong in data engineering skills with big data stack (Hadoop, Spark, Kafka, etc) Strong in transactional SQL, Enterprise Data Warehouse Experience with Graph Database, NoSQL databases Experience with Feature Engineering Experience with Master Data Management Experience with scripting languages: UNIX/Linux Shell, SQL, Python (Pandas, PySpark etc), Scala, R, etc ","Roles & ResponsibilitiesJob Purpose  The Data Engineer will provide big data engineering support to the Institutional Banking Group (IBG) Business Analytics Team in various data science projects. This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be one excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, Perform ETL/ELT, Data Modelling, Data Profiling, Data Cleansing, Feature Engineering tasks as part of Data Analytics Life Cycle (DALC); Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management ","$5,500to$11,000",Monthly,"Manager, Senior Executive",data engineer
,MOKA TECHNOLOGY SOLUTIONS PTE. LTD.,"Permanent, Full Time",Engineering,Data Engineer,"RequirementsYou have:  BS (MS preferred) in Computer Science or Computer Engineering. Excellent software engineering skills and proven track record (4+ years experience) in building automated, scalable and robust data processing systems. Proficiency in SQL, bash scripts and Python (or similar languages). Intermediate understanding of database technologies. Experience with data warehouse systems (e.g. Redshift) and batch/semi-online building blocks (e.g. MapReduce, Spark etc). Demonstrated expertise in working with large scale quantitative data. Excellent attention to detail and team player. ","Roles & ResponsibilitiesDo you have a passion for data? Are you looking to push the frontiers of innovation and build the Next Big Data Product? We are looking for excellent Data Engineers who are keen to help us manage the end-to-end data pipeline and drive big data solutions.  You will:  Design, implement and manage end to end data pipelines (ETL, data streaming and warehousing) so as to make data easily accessible for analysis. Integrate with third party APIs for accessing external data. Create and maintain data warehouses for reporting or analysis. Consult and partner with engineering and product teams to execute data-related product initiatives. Ability to quickly resolve performance and systems incidents. Evaluate the latest monitoring and automation tools. ","$5,000to$7,500",Monthly,Professional,data engineer
"KEPPEL TOWERS, 10 HOE CHIANG ROAD 089315",NTT DATA SINGAPORE PTE. LTD.,Contract,Information Technology,Data Engineer (Python developer),"RequirementsWe are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Information Systems. The candidate should also have experience using the following software/tools:  Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience building and optimizing 'big data' pipelines, data architectures and data sets. Experience with big data tools: Hadoop, Spark. Experience with object-oriented function scripting languages: Python, C++ Experience with statistical computer languages (Python, SQL, R) to manipulate data and draw insights from large data sets. Experience with common data science toolkits, such as NumPy, Pandas. R, etc. Knowledge of machine learning techniques and algorithms, such as K-NN, Naive Bayes, SVM, Decision Forests, etc. Experience with data visualisation tools, such as D3.js, GGplot, QlikView, etc. ","Roles & Responsibilities Own, develop and enhance the prototype cost allocation model for Singapore     Taking over the existing model and making further modifications / enhancements as required   Customise the prototype as required for rolling out the new cost allocation design to other countries     Implementing modifications / enhancements to the prototype as required for each country   ","$5,000to$7,500",Monthly,"Executive, Junior Executive, Senior Executive",data engineer
,CHANDLER MACLEOD GROUP PTE. LTD.,Contract,Information Technology,Mobile Network & Data Support Engineer,"RequirementsRequirements:  Bachelor degree in Telecommunications or related technical field   2+ to 5 years’ relevant experience in network engineering or telecommunications support environment Experience working in NOC environment Vast experience in IP networking, mobile networking, traffic analysis and troubleshooting    Ability to communicate effectively with a demonstrated ability to build valuable relationships  If this sounds like your next challenge get in touch -for a confidential discussion, please contact Naveen.Vasudevan@chandlermacleod.com Interested parties please click ""Apply Now"" or contact Naveen Vasudevan (EA Reg. No. R1330844) on +65 6429 3218 for more information. Chandler Macleod Group Pte Ltd, EA Licence: 11C3837","Roles & ResponsibilitiesA well-established global brand is looking for a high calibre Mobile Network and Data Support Engineer in Singapore. This role is responsible providing technical support to improve customer’s experience with front-line support team. Ideally you should have experience helping users to troubleshoot network connectivity issues, experience with NOC and highly knowledgeable with IP networking.   Responsibilities:  Deliver technical support inline with a set of departmental service levels through a scaled ticketing systems Perform product integrations with mobile operators remotely, involving traffic zero rating Work with the mobile infrastructure engineering team, finding or escalating issues (TCP/IP, VPN, SMPP, SS7) Analysing protocol logs, add new IP address and network traces Troubleshooting networking and service issues on Linux based operating systems Liaise with carrier / operator partners on technical issues around connectivity ","$4,000to$8,000",Monthly,Senior Executive,data engineer
,Company Undisclosed,Permanent,Engineering,DATA CENTER ENGINEER,"RequirementsBasic Requirements:     3+ years in Data Centers managing hardware assets   Familiar with Linux   Demonstrable project management experience with the ability to manage tasks under tight deadlines   Extensive experience as a data center engineer working within large heterogeneous environments   Extensive experience with rack and stack of server and network equipment installations   Solid day-to-day experience working data centers and co-location environments   Experienced working fast paced engineering data center environments   Solid experience managing teams of onsite infrastructure build-out engineers and vendors   Experience with a large variety of different vendor hardware such as servers, storage, and network equipment.   Familiar with the processes of pulling, terminating and testing copper and fiber network cabling   Experience in contributing to detailed project plans for programs involving cross-functional teams and challenging requirements   Ability to multitask and manage multiple projects   Demonstrated ability to coordinate issue resolution across departmental teams   Coordinating vendors, scheduling RMA's and on-site repairs as needed   Experience with Asset Management Systems   Experience in managing inventory of all cable, power cords and other infrastructure items.   Solid understanding of disaster recovery and business continuance methodology   Strong sense of methodology, process, and metrics     Preferred Requirements:   Bachelor's degree or equivalent work experience preferred   Proven track record of success and delivering results   Strong interpersonal and relationship building skills conducive to team development   Excellent communication skills, both verbal and written   Physical work Ability to lift at least 50-70 lbs on a regular basis  ","Roles & ResponsibilitiesAs a Data Center Engineer in the Technical Operations group, the ideal candidate will have the knowledge and experience to work in a fast-paced Data Center environment. They will be responsible for the day-to-day operations and projects to ensure the continued success.   Conduct all day-to-day operations for the Data Center   Install all racks and enclosures for equipment   Prepare all equipment to be installed in racks and other enclosures   Update cable the management system as changes are made to the data center cable plant   Update asset management system as equipment is added and removed from the Datacenter   Define tasks for each project and work with all TechOps teams to meet project timelines   Provide status to team members and management on the completion of all tasks   Provide all information to assist with Data Center capacity planning for space and power   Manage all vendor resources to complete tasks as defined in SOWs   Research new Data Center infrastructure equipment advancements and recommend changes as needed   Conduct all audits as required by company policies   Coordinate with the warehouse the delivery and shipping of all equipment    ",,,Senior Executive,data engineer
,DBS BANK LTD.,"Permanent, Full Time",Banking and Finance,"VP / AVP, Senior Data Engineer, Group Consumer Banking and Big Data Analytics Technology (180003L2)","Requirements Experience in big data and machine learning The ability to work with loosely defined requirements and exercise your analytical skills to clarify questions, share your approach and build/test elegant solutions in weekly sprint/release cycles. Development experience in Java/Scala and pride in producing clean, maintainable code Practical experience in clustering high dimensionality data using a variety of approaches Real world experience in solving business problems by deploying one or more machine learning techniques Experience creating pipelines to analyze data, extracted features and updated models in production. Independence and self-reliance while being a pro-active team player with excellent communication skills. Hands-on development with key technologies including Scala, Spark, and other relevant distributed computing languages, frameworks, and libraries.  Experience with distributed databases, such as Cassandra, and the key issues affecting their performance and reliability.  Experience using high-throughput, distributed message queueing systems such as Kafka. Familiarity with operational technologies, including Docker (required), Chef, Puppet, ZooKeeper, Terraform, and Ansible (preferred).  An ability to periodically deploy systems to on-prem environments.  Mastery of key development tools such as GIT, and familiarity with collaboration tools such as Jira and Confluence or similar tools.  Experience with Teradata SQL, Exadata SQL, T-SQL Strong experience in graph and stream processing Experience in migrating SQL from traditional RDBMS to Spark and BigData technologies Experience in building language parsers using ANTLR, query optimizers and automatic code generation In-depth knowledge of database internals and Spark SQL Catalyst engine ","Roles & Responsibilities Design and implement key components for highly scalable, distributed data collection and analysis system built for handling petabytes of data in the cloud.  Work with architects from other divisions contributing to this analytics system and mentor team members on best practices in backend infrastructure and distributed computing topics.  Analyze source data and data flows, working with structured and unstructured data. Manipulate high-volume, high-dimensionality data from varying sources to highlight patterns, anomalies, relationships and trends Analyze and visualize diverse sources of data, interpret results in the business context and report results clearly and concisely. Apply data mining, NLP, and machine learning (both supervised and unsupervised) to improve relevance and personalization algorithms. Work side-by-side with product managers, software engineers, and designers in designing experiments and minimum viable products. Build and optimize classifiers using machine learning techniques and enhance data collection procedures that is relevant for building analytic systems. Discover data sources, get access to them, import them, clean them up, and make them “model-ready”. You need to be willing and able to do your own ETL. Create and refine features from the underlying data. You’ll enjoy developing just enough subject matter expertise to have an intuition about what features might make your model perform better, and then you’ll lather, rinse and repeat. Run regular A/B tests, gather data, perform statistical analysis, draw conclusions on the impact of your optimizations and communicate results to peers and leaders. ","$7,000to$14,000",Monthly,"Middle Management, Manager",data engineer
51B CIRCULAR ROAD 049406,PALO IT SINGAPORE PTE. LTD.,"Permanent, Full Time",Information Technology,Senior Database Consultant - Big Data Engineer,"Requirements✔     You hold a Bachelor, Master or PhD degree in IT, Information Management and/or Computer Science ✔     You are just graduated or have less than 3 years of working experience ✔     Good knowledge of big data technology landscape and concepts related to distributed storage / computing ✔     Experience with big data frameworks (e.g. Hadoop, Spark) and distributions (Cloudera, Hortonworks, MapR) ✔     Experience with batch & ETL jobs to ingest and process data from multiple data sources ✔     Experience with NoSQL databases (e.g. Cassandra, MongoDB, Neo4J, ElasticSearch) ✔     Experience with querying tools (e.g Hive, Spark SQL, Impala) ✔     Experience or willingness to go in real-time stream processing, using solutions such as Kafka, Flume and/or Spark Streaming ✔     You are passionate about technology and continuous learning comes naturally to you  ","Roles & ResponsibilitiesYour profile & role on the project YOU:  Thrive on challenge. When was the last time you failed? Are curious & always learning. What are you up to right now? Can deal with constant change. When were you last surprised? Have mastered at least one skill of your trade but you’re not defined by it. What can you teach us? Can you wear many hats?  YOU AGAIN: The DevOps Architect will install, maintain, and support an on-premises cloud infrastructure and apply DevOps practices and solutions. The person will also implement cloud-related and DevOps technologies such as AWS/Puppet/Chef/Elk/Azure/Openstack. Other infrastructure related activities such as maintaining the company internal server infrastructure and respond to consultant requests when required will be expected.  Install, maintain, and support on-premises and off-premises cloud stack. Configure, maintain, and support the cloud-related infrastructures. Act as a system administrator on different OSes (e.g. RHEL, Opensolaris, Ubuntu, etc.) and help teams deploy their application and automate their development and releases on the cloud. Ability to develop solutions and self-learn new tools and technologies. Document, and share knowledge on developed DevOps solutions.  STILL YOU:  Unix / Linux / Bash knowledge Very good understanding of cloud computing (e.g. Technologies, Deployment, costing, HA/DR, etc.) Good understanding of DevOps principles (e.g. testing automation, BDD, TDD, Release automation, CI/CD, etc.) 2 years experience with cloud deployment (e.g. Openstack, VMWare, AWS, Azure, Terraform, etc.) 1 year experience with testing automation (e.g. Maven, Selenium, HP QC, LoadRunner) 1 year experience with release automation process (e.g. CA-RA, Jenkins, etc.) 1 year experience with Configuration Management (e.g. Ansible, SaltStack, Puppet/Chef, etc.) 1 year experience with monitoring tools (e.g. ELK, Prometheus, Grafana and Splunk) Experience with developing and implementing processes to handle releases from Development to Operations while respecting internal rules, and offering solutions for rollback) Experience with designing an architecture to implement development-to-production workflows. Knowledge of SRE, Containers, Kubernetes, Openshift is a plus. Good understanding of microservice architecture and DevOps practices that support. Strong RDMS and NoSQL skill in deploying and fine tuning such as MySQL, Oracle, Elasticsearch.  Your role at PALO IT You will be invited to take part in R&D works done within our Practices. You will have the chance to assist or be a speaker at must-attend international IT conferences. You will have the opportunity to write articles for our Blog or specialized press. Genuine ambassador of PALO IT, you will present our offers and take an active role in the development of the company.  Your technical environment # Cloud and DevOps based technologies (AWS/Puppet/Chef/Elk/Azure/Opencloud) # DevOps practices # Linux OS, Shell Scripting, SQL # Agile and scrum environment","$6,000to$12,000",Monthly,Professional,data engineer
,DBS BANK LTD.,"Permanent, Full Time",Banking and Finance,"VP / AVP, Machine Learning Engineer, Group Consumer Banking and Big Data Analytics Tech (180003YE)","Requirements PhD/Masters/Bachelors in Computer Science, Computer Engineering, Statistics, Applied Mathematics, or related disciplines.  Excellent understanding of software engineering principles and design patterns. Excellent programming skills in either Python, Scala, or Java. In-depth understanding of data science and machine learning technologies and methodologies. Good working knowledge of high performance computing, parallel data processing, and big data stack, e.g. Spark and Hadoop/Yarn. Experience to one or more commercial / open source data warehouses or data analytics systems, e.g. Teradata, is a big plus. Experience to one or more NoSQL databases is a big plus. Hands-on experience in Cloud platforms, e.g. AWS, or containerization/ virtualization platforms, e.g. Docker/Kubernetes, is a big plus. Experience to any data science or machine learning platform, e.g. IBM Data Science Experience or Cloudera Data Science Workbench, is a big plus. Exposure to mainframe system is a plus. Passion about machine learning and data-driven intelligence system. Excellent communication and presentation skills in English. Team player, self-starter, ability to work on multiple projects in parallel is necessary. 2+ years of experience in machine learning system or data science research 5+ years of experience in software engineering or DevOps automation or data engineering Experience working in multi-cultural environments ","Roles & ResponsibilitiesJob Purpose    Build and improve machine learning and analytics platform. Work with data scientists to create, optimize and productionize of machine learning models for various business units within the organization. Keep innovating and optimizing data and machine learning workflow to enable data-driven business activities at large scale.    Responsibilities   Build and improve machine learning and analytics platform.       Apply cutting edge technologies and tool chain in big data and machine learning to build machine learning and analytics platform. Keep innovating and optimizing the machine learning workflow, from data exploration, model experimentation/prototyping to production. Provide engineering solution and framework to support machine learning and data-driven business activities at large scale. Perform R&D on new technologies and solutions to improve accessibility, scalability, efficiency and us abilities of machine learning and analytics platform.     Work with data scientists to build end-to-end machine learning and analytics solution to solve business challenges.       Turn advanced machine learning models created by data scientists into end-to-end production grade system. Build analytics platform components to support data collection, exploratory, and integration from various sources being data API, RDBMS, or big data platform. Optimize efficiency of machine learning algorithm by applying state-of-the-art technologies, i.e. distributed computing, concurrent programming, or GPU parallel computing.      Establish, apply and maintain best practices and principles of machine learning engineering.       Study and evaluate the state of the art technologies, tools, and frameworks of machine learning engineering. Contribute in creation of blueprint and reference architecture for various machine learning use cases. Support the organization in transformation towards a data driven business culture.     Work Relationships  Internal        Work closely with data scientists, business team, and project managers to provide machine learning and data-driven business solution.  Collaborate with other technology teams to build platform and framework to enable machine learning and data analytics activities at large scale     External        Maintain engineering principles and best practices of machine learning framework and technologies.   ","$7,000to$14,000",Monthly,"Middle Management, Manager",data engineer
"ENTREPRENEUR BUSINESS CENTRE, 18 KAKI BUKIT ROAD 3 415978",ENGIE ITS  PTE. LTD.,Permanent,Engineering,Data Centre Facilities Management Engineer,"RequirementsRequirements:  Diploma / ITC Cert in Information Technology, Building Services, Electrical or Mechanical Engineering At least 1 years experience with Facility Management or M&E facilities maintenance, preferably in data centre environment Service-oriented with strong analytical and problem-solving skills Resourceful, dynamic, highly motivated and able to work independently Good interpersonal skills with ability to interact with people at different levels Willing to work overtime including weekends and public holidays when necessary  Personal Attributes  Meticulous and have an eye for details with positive working attitude Organized and details-oriented, with a strong focus on accuracy Able to work under pressure and tight deadline  Other Information:  Job Type: Full Time, Permanent Work week: 5 days Work location : Various location ","Roles & ResponsibilitiesResponsibilities: Manage data centre operations and facilities Plan and implement predictive and preventive programmes Manage sub-contractors to carry out maintenance works Project manage facility development Provide engineering/technical expertise and value engineering to customers Respond to service call and ensure resolution of the problems Prepare weekly reports, incident reports and O&M procedures Ensure critical system availability to meet SLA.","$2,300to$4,000",Monthly,Junior Executive,data engineer
"PENINSULA PLAZA, 111 NORTH BRIDGE ROAD 179098",Company Undisclosed,Contract,Information Technology,Big Data Engineer,"RequirementsRequired to work European time zone (4pm-1am) • Degree qualified in Business management, IT, Computer Systems, software or computer engineering fields or equivalent. • Minimum 6 years of experience in data warehousing / big data environments. • Experience with big data processing  • Experience in designing and developing data models, integrating data from multiple sources, building ETL pipelines, and other data wrangling tools in big data environments • Understanding of structured and unstructured data design/modeling • Experience using software engineering best practices in programming, testing, version control, agile development, etc.     Technical competency: • Hadoop / Big Data knowledge and experience • Design & Development based on Hadoop platform and it’s components • Big Data Platform based on Cloudera on Hadoop • Python / Spark / Scala / Java • HIVE / HBase / Impala / Parquet • Sqoop, Kafka, Flume • SQL • Relational Database Management System (RDBMS) • NOSQL database • Data warehouse platforms or equivalent   Essential skill set: • Highly organized, self-motivated, pro-active, and able to plan. • Ability to analyze and understand complex problems. • Ability to explain technical information in business terms. • Ability to communicate clearly and effectively, both verbally and in writing. • Strong in User Requirements Gathering, Maintenance and Support. • Agile experience in a Scrum setting • Data Architecture, Data Modeling of BI Applications / Data Warehouse / Big Data","Roles & Responsibilities• Evaluate and renew implemented big data architecture solutions to ensure their relevance and effectiveness in supporting business needs and growth. • Design, develop and maintain data pipelines, with a focus on writing scalable, clean, and fault-tolerant code to handle disparate data sources, process large volume of structured / unstructured data from various sources. • Understand business requirements and solution designs to develop and implement solutions that adhere to big data architectural guidelines and address business requirements • Support and maintain previously implemented big data projects, as well as provide guidance and consultation on other projects in active development as needed • Drive optimization, testing and tooling to improve data quality • Document and communicate technical complexities completely and clearly to team members and other key stakeholders","$6,500to$9,000",Monthly,Professional,data engineer
"ANSON CENTRE, 51 ANSON ROAD 079904",INTELLECT MINDS PTE. LTD.,Full Time,Information Technology,Data Engineer,"RequirementsQualifications • Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. • Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. • Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. • Strong analytic skills related to working with unstructured datasets. • Build processes supporting data transformation, data structures, metadata, dependency and workload management. • A successful history of manipulating, processing and extracting value from large disconnected datasets. • Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. • Experience supporting and working with cross-functional teams in a dynamic environment. • We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: • Experience with big data tools: Hadoop, Spark, Kafka, etc. • Experience with Google Cloud Platform esp. Google Pub-Sub, Big Query, Data Proc, Data Flow, Cloud Storage. • Experience with IoT & Time series data. • Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. • Experience with stream-processing systems: Storm, Spark-Streaming, etc. • Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. All successful candidates can expect a very competitive remuneration package and a comprehensive range of benefits. Interested Candidates please submit your detailed resume online. To your success! The Recruitment Team Intellect Minds Pte Ltd (Singapore)","Roles & ResponsibilitiesCompany Overview Intellect Minds is a Singapore-based company since 2008, specializing in talent acquisition, application development, and training. We serve BIG MNCs and well-known clients in talent acquisition, application development, and training needs for Singapore, Malaysia, Brunei, Vietnam and Thailand. Our client is an establish company a, leader within their industry, is now looking for a Data Engineer to join their esteemed organization. Job Descriptions: Responsibilities • Create and maintain optimal data pipeline architecture. • Assemble large, complex data sets that meet functional / non-functional business requirements. • Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Cassandra, Hadoop and other Big Data Technologies. • Build data pipeline on premise and on Google Cloud Platform. • Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. • Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. • Work with data and analytics experts to strive for greater functionality in our data systems.","$5,000to$7,000",Monthly,Executive,data engineer
"AIA TOWER, 1 ROBINSON ROAD 048542",AIA SINGAPORE PRIVATE LIMITED,Full Time,Information Technology,Data Engineer,"Requirements A background in Computer Science, Engineering, Mathematics or Statistics. 1 to 2 years of experience building complex data pipelines for data integration from enterprise wide applications/systems into centralised big data lakes/warehouses Open to fresh graduates who possess strong programming skill in java / python, SQL and Shell scripting ","Roles & Responsibilities You will be designing, developing and testing ETL Mappings, Mapplets, Workflows and Worklets You will be developing data pipelines to extract, transform and aggregate data that can scale to petabytes, elastically, with low latency and high availability. ","$3,000to$6,000",Monthly,Executive,data engineer
"VISION EXCHANGE, 2 VENTURE DRIVE 608526",SILOT PTE. LTD.,Full Time,Sciences / Laboratory / R&D,Data Platform Engineer,"RequirementsSkills & Qualifications   Requirements:  Excellent computer science knowledge Working experience in designing and building big data platforms (distributed filesystems, distributed databases, batch processing frameworks, stream processing frameworks, message queues, job scheduling) Expert knowledge of scalable architecture for real-time processing Experience ensuring data quality and compliance Experience with DevOps and continuous integration Experience in agile practices, test-driven development Fluent written and spoken English  Good to have:  Good documentation skills Experience in Fintech Experience with machine learning ","Roles & ResponsibilitiesRole   As data engineer at Silot, you design and implement distributed backend services that processes data in real-time, with focus on scalability, data quality and integration of machine learning.   You feel natural at extracting information out of heterogeneous data from many sources. You can plan, build and maintain distributed, service-oriented and event-driven data platform for real-time processing. You like delivering accurate data components that people rely on. You optimize architecture and processes for performance and stability.   To visualize what this position is like, think ""building systems,"" not ""processing data"" (even though your day will involve aspects of both).","$4,000to$7,000",Monthly,Professional,data engineer
90 EU TONG SEN STREET 059811,TITANSOFT PTE. LTD.,Permanent,Professional Services,Senior Data Engineer,"RequirementsWhat we are looking for in a Senior Data Engineer Qualifications  BA / BS in Computer Science, Electronics or Electrical Engineering, Information Technology or other relevant fields  Experience  3+ years of experience in Unix / Linux operation systems (e.g. file systems, inodes, system calls) or networking (e.g. TCP / IP, routing, network topologies and hardware, SND) 2+ years of hands-on experience in the data warehouse space, custom ETL design, implementation and maintenance 3+ years of hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred) Experience with large data sets, Hadoop, and data visualization tools  Skills  Strong data architecture, data modeling, schema design Effective project management skills in leading data driven projects from definition to interpretation and execution Ability to initiate and drive projects, and communicate data warehouse plans to internal clients / stakeholders  What makes a (Super!) Senior Data Engineer in Titansoft  Expertise in designing and analyzing large-scale distributed systems (e.g. Hadoop, Kafka, Hive) Systematic problem-solving approach Ability to debug and optimize code and automate routine tasks ","Roles & ResponsibilitiesIf you believe data makes the world go round, we believe we have found the one we are looking for. Our data and research team are the ultimate magicians of data. They throw data in the system, wave their hands around the keyboard, and pull out a never-ending stream of business value. Our team have opportunities to build efficient and reliable data pipelines that move data across systems. Our team are part mathematician, part computer scientist, and part interpreters- magicians of data. If you are interested to work some magic with our data, drop us an owl. What a Senior Data Engineer does in Titansoft  Partner with internal stakeholders to understand business requirements Work with cross-functional data and product teams to build efficient and scalable data solutions Design, build, optimize, launch and support new and existing data models in production Build scalable solutions of real-time data streaming and static analysis Setup network for deploy cluster and troubleshooting Write Linux script programming to assist in auto deploy and system health monitoring Design and build reliable Hadoop system ","$4,000to$8,000",Monthly,Executive,data engineer
137 TELOK AYER STREET 068602,WOODPECKER ASIA TECH PTE. LTD.,Permanent,Information Technology,Data Engineer,"RequirementsSkills:  2-3 years of working experience as a Data Engineering or as a developer Bachelor’s or Master’s degree in Computer Science Proficient in Java and in one of the scripting languages such as Python, JavaScript, Ruby or PHP Proficient with No-SQL databases like HBase or MongoDB Proficient in ETL processes and programming models on Apache Beam Proficient in cloud computing systems management services such as Stackdriver Familiar with cloud services such as MS Azure, Google Cloud Platform or AWS Familiarity with Machine Learning and Statistical techniques for data mining will be beneficial Familiarity with Google Analytics and Google Tag Manager will be an advantage Proficient in oral and written communication in English, as well as effective interpersonal skills ","Roles & ResponsibilitiesOur Data Engineer (in the analytics team) is the person responsible for enhancing our analytics and performance management framework. You’ll be building our data infrastructure - like databases and large-scale data processing tools -  on the Google Cloud Platform.  You’ll be a perfect fit in this role if you’re an eager learner, have prior experience in quantitative domains, and if you’re keen to be a team player in a dynamic start-up. You’ll get to:  Design, construct, install, test and maintain highly scalable data management systems Employ a variety scripting languages and tools to marry systems together Make sure our systems meet business requirements and industry practices Research opportunities for data acquisition and new uses for existing data Develop data set processes for data modelling, mining and production Integrate new data management technologies and software engineering tools into existing structures Create custom software components and analytics applications Install and update disaster recovery procedures Recommend ways to improve data reliability, efficiency and quality Collaborate with data architects, modelers and IT team members on project goals Build high-performance algorithms, prototypes, predictive models and proof of concepts ","$4,000to$8,000",Monthly,Professional,data engineer
90 EU TONG SEN STREET 059811,TITANSOFT PTE. LTD.,Permanent,Professional Services,Data Engineer,"RequirementsWhat we are looking for in a Data Engineer Qualifications BA/BS in Computer Science, Electronics or Electrical Engineering, Information Technology or other qualified achievement Experience Hands-on experience in SQL or similar languages and development experience in at least one scripting language (Python preferred) Understand data architecture, data modeling and schema design What makes a (Super!) Data Engineer in Titansoft Patience to clean up huge amounts of data Passion to research domain knowledge Experience with large data sets, Hadoop, and data visualisation tools Interest in cloud computing and service (GCP, AWS, Azure) Interest in AI/ Machine-Learning product","Roles & ResponsibilitiesIf you believe data makes the world go round, we believe we have found the one we are looking for. Our data and research team are the ultimate managers of data. Others see meaningless figures but they see value. Our team are part of mathematician, part of computer scientist, and part of interpreters. Of data. What a Data Engineer does in Titansoft Manage data warehouse with plans for a business vertical or a group of business verticals Generate and manage all allocated data sets including ensuring its quality based on requirements Work with our Data Infrastructure team to triage and resolve infrastructure issues Manage the delivery of high impact dashboards and data visualisation diagrams","$3,000to$8,000",Monthly,Executive,data engineer
"ROBINSON SQUARE, 144 ROBINSON ROAD 068908",BLUE STAR INFOSTACK SOLUTIONS PTE. LTD.,Contract,Information Technology,Data Engineer,"Requirements 6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs   ","Roles & ResponsibilitiesData Engineer  – Location Singapore    6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs   ","$3,500to$6,000",Monthly,Executive,data engineer
"SINGAPORE LAND TOWER, 50 RAFFLES PLACE 048623",CIMB BANK BERHAD,Permanent,Information Technology,"Manager (Data Engineer), Decision Science, Credit Cards & Personal Financing","Requirements Bachelor Degree / Diploma in Computer Science, Information System or related studies. Minimum 5 years of experience in a data processing role. Consistent track record of designing and implementing scalable, performant data pipelines, data services, and data products. Proven ability to work well independently and within a fast-paced, collaborative environment. Excellent debugging, critical thinking and communication skills. Programming experience in building high quality data applications. Skills such as Java, Python or ASP.NET are preferred. Proficient in SAS, MS Excel and SQL Programming. Strong aptitude for learning new technologies related to Data Management and Data Science.  Please send detailed resume, including salary expectation and contact number to sg.enquiries@cimb.com. We regret that only shortlisted candidates will be notified.","Roles & Responsibilities Design, implement, and optimize the data pipelines from various channels using the latest technologies. Deploy dimensional data model to support functional and analytical business requirements. Partner with data analysts, business subject-matter experts and cross-functional technology teams to deliver end-to-end analytics solutions. Determine reporting needs to be integrated with business intelligence visualization tools and manage reports distribution via content management tools. Research, evaluate, and recommend technical solutions for data collection, processing, and reporting. Data cleansing, scraping unstructured data and converting them into structured/usable data. ","$5,000to$6,000",Monthly,Manager,data engineer
"ROBINSON SQUARE, 144 ROBINSON ROAD 068908",INFOGAIN PTE. LTD.,Contract,Information Technology,Data Engineer,"Requirements  – Location Singapore    6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs  ","Roles & ResponsibilitiesData Engineer  – Location Singapore  6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs   ","$3,500to$6,000",Monthly,Executive,data engineer
"COMCENTRE, 31 EXETER ROAD 239732",DATASPARK PTE. LTD.,Permanent,Information Technology,Data Engineer,"RequirementsRequirements  7+ years of superior software development experience building commercial large-scale software systems and database systems Excellence in algorithms, data structure, discrete math, data base and data warehousing Expert knowledge in data management technologies and software engineering tools to efficiently process large volume of data Demonstrated clear and thorough logical and analytical thinking, as well as problem solving skills Experience of data warehouses in excess of 10TB Experience of Web UI, middle tier, and data back end development Production coding experience in choice of programming languages and development frameworks Proven professional experience in processing large-scale commercial data. Experience with telco data a plus. Superior and proactive communications skills, including verbal, written, and presentation. A proven team player and contributor. Self-directed, ability to work independently and research innovative solutions to business problems Aptitude of working on multiple projects in parallel Attention to details and data accuracy MS or BS degree in Computer Science/Engineering, Statistics, Mathematics, or equivalent is required for this position. ","Roles & ResponsibilitiesResponsibilities  design and implement scalable and robust software platform for ingesting and transforming telco network datasets in (near) real-time using a variety of open-source and proprietary Big Data technologies recommend and implement ways to improve data reliability, efficiency and quality collaborate with product management, sales and marketing, and solution delivery teams to support the objectives that customer requirements are well managed and reflected in product releases support the deployment of DataSpark software within clients' IT environment working closely with stakeholders to ensure high standards of data governance during implementation serve as technical subject matter expert in latest big data technologies ","$3,500to$10,000",Monthly,"Executive, Senior Executive",data engineer
"COMCENTRE, 31 EXETER ROAD 239732",DATASPARK PTE. LTD.,Permanent,Information Technology,Lead Data Engineer,"RequirementsRequirements  10+ years of superior software development experience building commercial large-scale software systems and database systems Excellence in algorithms, data structure, discrete math, data base and data warehousing Expert knowledge in data management technologies and software engineering tools to efficiently process large volume of data Demonstrated clear and thorough logical and analytical thinking, as well as problem solving skills Experience of data warehouses in excess of 10TB Experience of Web UI, middle tier, and data back end development Production coding experience in choice of programming languages and development frameworks Proven professional experience in processing large-scale commercial data. Experience with telco data a plus. Superior and proactive communications skills, including verbal, written, and presentation. A proven team player and contributor. Self-directed, ability to work independently and research innovative solutions to business problems Aptitude of working on multiple projects in parallel Attention to details and data accuracy MS or BS degree in Computer Science/Engineering, Statistics, Mathematics, or equivalent is required for this position. ","Roles & ResponsibilitiesResponsibilities  design and implement scalable and robust software platform for ingesting and transforming telco network datasets in (near) real-time using a variety of open-source and proprietary Big Data technologies recommend and implement ways to improve data reliability, efficiency and quality collaborate with product management, sales and marketing, and solution delivery teams to support the objectives that customer requirements are well managed and reflected in product releases support the deployment of DataSpark software within clients' IT environment working closely with stakeholders to ensure high standards of data governance during implementation serve as technical subject matter expert in latest big data technologies ","$3,500to$10,000",Monthly,"Executive, Senior Executive",data engineer
"COMCENTRE, 31 EXETER ROAD 239732",DATASPARK PTE. LTD.,Permanent,Information Technology,Senior Data Engineer,"RequirementsRequirements  7+ years of superior software development experience building commercial large-scale software systems and database systems Excellence in algorithms, data structure, discrete math, data base and data warehousing Expert knowledge in data management technologies and software engineering tools to efficiently process large volume of data Demonstrated clear and thorough logical and analytical thinking, as well as problem solving skills Experience of data warehouses in excess of 10TB Experience of Web UI, middle tier, and data back end development Production coding experience in choice of programming languages and development frameworks Proven professional experience in processing large-scale commercial data. Experience with telco data a plus. Superior and proactive communications skills, including verbal, written, and presentation. A proven team player and contributor. Self-directed, ability to work independently and research innovative solutions to business problems Aptitude of working on multiple projects in parallel Attention to details and data accuracy MS or BS degree in Computer Science/Engineering, Statistics, Mathematics, or equivalent is required for this position. ","Roles & ResponsibilitiesResponsibilities  design and implement scalable and robust software platform for ingesting and transforming telco network datasets in (near) real-time using a variety of open-source and proprietary Big Data technologies recommend and implement ways to improve data reliability, efficiency and quality collaborate with product management, sales and marketing, and solution delivery teams to support the objectives that customer requirements are well managed and reflected in product releases support the deployment of DataSpark software within clients' IT environment working closely with stakeholders to ensure high standards of data governance during implementation serve as technical subject matter expert in latest big data technologies ","$3,500to$9,000",Monthly,"Executive, Senior Executive",data engineer
,ALPHATECH BUSINESS SOLUTIONS PTE. LTD.,Permanent,Information Technology,Data Quality Engineer,"Requirements   Must know JAVA8 and SPARK Experience in distributed data architecture Have working knowledge of SQL, Python, Airflow Scala, Hadoop, SPARK Good to know CI/CD Experience (Jenkins Github), AWS, Kubernetes, Docker Preferred to have banking domain experience ",Roles & Responsibilities Evaluate and recommend solutions via data analysis regarding issues related to the improvement of product qua;oty and resolving of customer feedback Apply software and programming abilities to manage and analyse data from a variety of sources ,"$6,000to$7,800",Monthly,Senior Executive,data engineer
,A*STAR RESEARCH ENTITIES,"Contract, Full Time",Sciences / Laboratory / R&D,"Research Engineer (Big Urban Data), IHPC","Requirements Master/Bachelor Degree in mathematics, engineering and electronics Good programming experiences and capabilities in python, c sharp, R and Java Experience in data analysis, parallel computing, optimization, agent-based simulation, and/or visualization is an added advantage ",Roles & ResponsibilitiesHe/She will mainly work on data preprocessing and preliminary analysis for dengue risk modeling research in this project. He/She will also be responsible for database construction and maintenance in this project and assist research scientist to fulfil the research tasks and deliverables.,"$2,500to$5,000",Monthly,Non-executive,data engineer
,Company Undisclosed,"Contract, Full Time",Information Technology,Data Security Engineer (Ref 22641),"Requirements- Degree in Information System or Computer Science related  - Must have knowledge in Protegrity and/or Blue Talon software  - 3 to 4 years of experience in banking environment  - Preferably trained in Agile methodology  - Proficient in Jira  - 3+ years of hands on experience in distributed data architectures is a must - 3+ years of Core Java or Scala or Python is a must  - Working knowledge of Apache Spark is a must  - CI/CD experience (Jenkins, Github) is a must  - Working knowledge of Bash scripts is a must  - AWS experience is nice to have  - Kubernetes or Docker experience is nice to have - Strong communication skills  - Previous work experience in Big Data environment is a plus - Understand Agile and full data warehouse flow - Understand database languages for analysing data out of data warehouse   Licence No: 12C6060","Roles & Responsibilities- Integration experience with backend product and booking engines - Creating complex applications, transforming user experience and enterprise - Working with some of the latest tools and techniques  - Hands-on coding, initially solely responsible developing services that construct applications front to back ie from UX to data acquisition and repositories  - Working in highly collaborative teams and building quality code - Working with senior business and technical colleagues to rapidly deliver solutions - Collaborating with and working under the direction of senior technical colleagues - Contributing to the technical design and architecture, particularly in service / microservice decomposition  - Knowledge in lots of different domains, programming languages and client environments - Furnish the business domain deeply and working closely with business stakeholders","$3,000to$6,000",Monthly,Executive,data engineer
"WESTGATE TOWER, 1 GATEWAY DRIVE 608531",DAIMLER SOUTH EAST ASIA PTE. LTD.,Full Time,Information Technology,Assistant Manager - Data Engineer,"Requirements• Bachelor’s Degree in Business or IT Qualification • 2+ years’ relevant experience, minimum 5 years of working experience • Specific knowledge: People Management knowledge; IT knowledge; Process knowledge       • Experience with Azure Data Lake, Azure Data Lake Analytics, Azure Data factory, HDInsights. • Experience with design and implementation of HDInsights with Spark, Kafka clusters. • Experience with SQL Server Integration Services, Reporting Services and Analytic Services. • Familiar with Microsoft Azure data storage, ingestion, computation services & APIs. • Understands the common data movement architectures (like ETL, ELT, etc.) • Strong experience with Metadata Management, establish sourcing and access patterns for enterprise reference data. • Experience with data extraction and manipulation, and ad-hoc query tools • Highly motivated independent worker with minimum guidance required; • Readiness to travel; (20% of travel) • Ability to work in an international and intercultural context • Excellent communication skills in both written and spoken. Multi-linguistic abilities would be an added advantage.","Roles & Responsibilities• Responsible for managing data sources, Data Ingest and Data Preparation processes including Metadata Management • Responsible for design of data models and to ensure they are in alignment with headquarter and global data models and scalable to other regions • Participate in Data Requirements Gathering and Analysis meetings with Team Members, Internal Customers, and Stakeholders. • Develop, maintain, test, and troubleshoot data solutions, including Database Development, ETL / Data Migration Development, and Big Data Development.   1. IT Solutions Delivery • Analyse complex, high-volume, high-dimensionality data from varying sources using a variety of ETL and data analysis techniques. • Responsible for managing data sources, Data Ingest and Data Preparation processes including Metadata Management • Responsible for design of data models and to ensure they are in alignment with headquarter and global data models and scalable to other regions • Participate in Data Requirements Gathering and Analysis meetings with Team Members, Internal Customers, and Stakeholders. • Develop, maintain, test, and troubleshoot data solutions, including Database Development, ETL / Data Migration Development, and Big Data Development. • Understanding and support of Regional overseas Data Lake and analytics engine and platform • Assist with enabling “Next best action / Offer and data insights” solutions that are scalable and transferable for regional markets • Investigation and understanding data landscape for the respective markets • Ability to recognize/analyse highly complex processes, interdependencies and gaps and to develop new approaches to solutions • Supports and troubleshoots the data movement processes and the data warehouse environment • Promote synergies and reuse within and across projects and platforms in order to maximize rapid yet responsible delivery   2. IT Strategy and Strategic Supplier Management (SSM) • Support in lean implementation and managing run costs downwards. • Support in sourcing and supplier management, liaise with ITx on SSM • Sharing and collaboration of solution across all regions • Ensure that standardized solutions are delivered • Ensure consistency of the overall Overseas IT landscape • Conduct post-implementation assessment to ensure that required value was delivered   3. Coordination & Communication • Management reporting and provide training on the platform architecture and strategy topics • Build-up and strengthen relationship with all stakeholders in the region, headquarter and the markets to effectively perform the necessary tasks within the function • Ability to explain complex processes clearly and precisely to different target groups and to explain the specific benefits of solution approaches  ","$6,000to$13,000",Monthly,Professional,data engineer
"NEXUS @ONE-NORTH, 1 FUSIONOPOLIS LINK 138542",FOX NETWORKS GROUP SINGAPORE  PTE. LTD.,Permanent,Information Technology,"Data, Analytics & Marketing Technology Engineer","RequirementsMust-haves  Minimum 3 years working knowledge with any scripting language (i.e. Javascript/Node, Python, Go) Minimum 3 years working knowledge of any RDS such as PostgreSQL, MySQL. Understands various network transport protocols, and data file formats. Experience with implementing Client SDKs in a mobile environment. Experience with Implementing Database infrastructures in AWS using RDS, elasticache, redshift, and aurora. Proficient understanding of distributed computing principles Experience with the Implementation & Management of Amazon Redshift Working knowledge of ETL in a Data Warehousing Context Experience with the concept of analytics aggregation, or operation of tools such as Segment.com Experience with at least one of these tools: PowerBI, Tableau, Google Analytics, or New Relic Insights. Experience with the implementation & Management of Marketing platforms, e.g. Braze, Adobe, Oracle    Ideal & Preferred  Certification in any enterprise RDS programmes. Experience with NoSQL databases such as HBase, Cassandra, or MongoDB Working knowledge of AWS Lambda. Proficiency with Implementing and Maintaining Hadoop v2, MapReduce, or HDFS Experience in Spark, Pig, Hive, or Impala Experience with various messaging systems, such as Kafka or RabbitMQ Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming Experience with Big Data ML toolkits, such as Mahout, SparkML, or H2O Experience with Cloudera/MapR/Hortonworks ","Roles & ResponsibilitiesThe Role: We are looking for a DevOps Engineer (Data, Analytics & Marketing Technology) that will work on the integration & improvement of our Analytics, Business Intelligence, and Customer Marketing platforms. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company.   Reporting To:  Head of Product, FOX+ (Asia)   Key Responsibilities:   Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities Working with the Business Intelligence team to develop & maintain solutions for data consolidation, surfacing, and visualisation of Analytics & Data Translating business requirements from Marketing, Content, Product & Business Intelligence teams into technical specifications for implementation by relevant engineering teams Managing implementation & integration of analysis & visualisation tools Owning the technical definition & best practice for Analytics & tracking implementation into a suite of applications incl. Mobile, web & leanback. Implementing ETL process Monitoring performance and advising any necessary infrastructure changes Defining data retention policies   ",,,Manager,data engineer
,Company Undisclosed,Full Time,"Engineering, Information Technology, Repair and Maintenance, Telecommunications",Data Center Engineer,Requirements    at least diploma with min. 2years experience in Data Center  3 rotating shifts ,"Roles & Responsibilities knowledge/familiar in Network device/cabling /server/cabinet/rackesponsible for designing, setting up, and managing information/network systems monitoring systems operations and administering IT solutions to ensure servers, hard drives, and other data center equipment function efficiently. carry out performance-tuning operations on data center storage systems to ensure high level of data quality, availability, and security. ","$3,000to$4,500",Monthly,"Middle Management, Fresh/entry level",data engineer
"AXA TOWER, 8 SHENTON WAY 068811",LAZADA SOUTH EAST ASIA PTE. LTD.,Permanent,Information Technology,"Manager, Data Engineer",Requirements Bachelor’s Degree in Computer Science/Information Engineering or relevant technical field experience Minimum of 3 years of related professional work experience A great communicator Love code and open to learn new technologies Understand how Big Data databases works and how to get the information from it in a efficient way A decision maker. You will work in a environment with many stakeholders and you will have to understand and make decision in based of tradeoffs  ,Roles & ResponsibilitiesWe are looking for a Data Engineer that will be responsible to gather public information from the different players in the e-Commerce industry and consolidate them into a dashboard to get insights. You will be conducting strategic analysis to improve the products that are promoted on Lazada. What you will do:  You will work with Big Data information - structuring different inputs into a dashboard that is understandable for local category teams to action on You will work with APIs and bots technology You will be the middleman between Business requirements and implementation You will improve the Lazada platform to get information from different websites and social networks ,"$5,000to$8,000",Monthly,Manager,data engineer
"AIRLINE HOUSE, 25 AIRLINE ROAD 819829",SINGAPORE AIRLINES LIMITED,Permanent,Information Technology,Data Sciences & Analytics Engineer,"Requirements BS in Computer Science, Mathematics, Statistics, Physics or related discipline is required. Advanced degree related to analytics, machine learning or AI is preferred. Intermediate or advanced programming skills in Python. Conversant with algorithm design, data structure and SQL. Functional/object-oriented software development experience using Java or Scala is a plus. At least 2 years of relevant industry experience in two or more of the following areas: 	 Hands-on skills in shallow machine learning, AI, or information retrieval. Experience in GPU-accelerated deep learning frameworks (such as Keras, TensorFlow, PyTorch) is a plus. Knowledge and working experience in workflow, map-reduce or stream processing systems such as Spark and Kafka. Knowledge in statistics, especially Bayesian statistics and inference. Knowledge and working experience with data visualization tools like Tableau, PowerBI and Qlik.   Experience with Agile/Scrum/Kanban methodologies is a plus. Hands-on experience with AWS, GCP or similar public cloud environment is a plus. Excellent interpersonal & communication skills to work with non-technical business users. Proven ability as a problem-solver.  ","Roles & ResponsibilitiesSIA has multiple positions for junior and senior data scientists to drive our business analytics, data science and AI initiatives. Responsibilities include the following:  Member of an in-house data analytics and AI development team that works on machine learning (including NLP, image recognition, recommender system, deep learning), experimental design, and optimization. Oversee the technical work and provide datasets to external technology partners to deliver products/services in data analytics, data science and AI. Support business users in the assessment/validation of partner-supplied prediction models and in their deployment to production. Help business units create Tableau dashboards with relevant datasets. Extract insights through data visualization. Work closely with application development teams to operationalize and integrate analytics/machine learning capabilities into production systems using RESTful-API microservices. ","$4,000to$8,000",Monthly,"Professional, Executive",data engineer
"PRUDENTIAL TOWER, 30 CECIL STREET 049712",ITCAN PTE. LIMITED,Full Time,Information Technology,Data Engineer - MSBI Stack,"Requirements. Bachelor’s degree or equivalent in relevant discipline. 2. Minimum 5 yrs. exp. working on MSBI Stack (SSAS, SSIS, SSRS) 3. Minimum 5 yrs. exp. working on SQL Server, DB2, Oracle databases 4. Working knowledge of modeling tools (Erwin, Embarcadero, EA Sparx etc.) in addition to hands-on experience with business intelligence tools 5. Mandatory Technical expertise in:  MS SQL Server 2008 R2 - FastTrack Data Warehouse Databases - DB2, SQL Server, Oracle (Must be able to write complex queries against these DBs) SQL Server Integration Services (Writing efficient SSIS packages) Any data replication tool (preferred: MS SQL Server, DT Share) Writing complex & efficient Stored Procedures, Views, Functions in MS SQL Server Optimizing complex SQL queries for optimum performance Setting up schedule jobs using Windows Scheduler or SQL Agent Any Data Quality Tools - Preferred: TIBCO Trillium, MS DQS Any Master Data Management Tools - Preferred: TIBCO MDM, MS MDS  6. Technical expertise in the following areas are a plus:  ETL experience with SAS Enterprise Guide ","Roles & Responsibilities1. Designs and creates information store / data warehouse and all related extraction, transformation and load of data functions 2. Experts at taking a big-picture view of a company's data situation 3. Proficient in able to read, analyze and digest what a business wants to accomplish with its data, understand data models and translate them into the best possible design of Databases and ETL processes 4. Executes basic and advanced transformation activities such as normalization, cleansing, aggregation, summarizing, and integration 5. Designs automation processes to control data access, transformation and movement; ensures source system data availability and update accessibility, data integrity, restorability and appropriately handles errors in a timely manner 6. Develops logical and physical data flow models for ETL applications. Translates data access, transformation and movement requirements into functional requirements and mapping designs 7. Plans and conducts ETL unit and development tests; monitors results and takes corrective action, when necessary 8. Plans, coordinates and implements security measures to safeguard data in information store / datawarehouse against accidental or unauthorized damage, modification or disclosure 9. Analysis of business requirements document created by Data Architect team 10. Create SSIS packages based on mapping document 11. Unit test & optimize SSIS package according to business needs 12. Document technical details of ETL project in Excel & word documents 13. Code maintenance in TFS (Branching/Merging) 14. Contact Data Architects for any queries/confusions/design changes for IS","$5,000to$7,000",Monthly,Professional,data engineer
"AXA TOWER, 8 SHENTON WAY 068811",HCL SINGAPORE PTE. LTD.,Permanent,Information Technology,Data Centre Operations Engineer,RequirementsHands on with DC infrastructure environment Good Experience in DC 24*7 Operation process & procedures DC racking and stacking & structured cabling understanding GOOD to HAVE Any Technical certification DC Certification Technical / Professional Skills Experience working with external Vendors Good communication and interpersonal skills Non-Technical / Soft Skills Excellent team player flexibility in 24*7 environment Maximum availability for operational requirements  ,"Roles & ResponsibilitiesThe Data center hands and feet engineer will Perform day to day activities, users, visitors and subscriber’s co-ordination. Co-ordinate with visitors visiting the DC for activities Will co-ordinate with respective teams with in a project Will be responsible for handling issues pertaining to operations and production Will be responsible to manage incidents during the shift Will be part of DC hands and feet operation team to work for customer on different DC member sites (Singapore) in the project and performing equipment Installation and Decommission, troubleshooting. Travel to different sites for INC/Planned activities Will be installing and decommissioning equipment’s, servers racking and stacking in the Datacentre Will be performing DC Cabling, labelling, patching Daily DC server health checks, walkthroughs, reporting, printing, remedy and email queue management Must have DC Cabling and equipment hardware troubleshooting knowledge Should have basic understanding of DC facilities, environment and standards Receiving the material deliveries, maintaining the inventory store room Maintaining visitor records & access logs 24*7 environment, shift rotation Flexibility per team rotation requirement Professional in 24/7 operational team availability  ","$4,500to$5,000",Monthly,Professional,data engineer
"PRUDENTIAL TOWER, 30 CECIL STREET 049712",ITCAN PTE. LIMITED,"Contract, Full Time",Information Technology,Data Center Engineer,"Requirements One year and more working experience in server room operation and maintenance is preferred. Related knowledge of computer foundation, server room operation, infrastructure and strong and weak current.  Have data analysis and report generating skills  Willing to work base on shift  ","Roles & Responsibilities1.       To take care of the function of the external service of the server room, which includes floor plan and layout of the server room, electrical equipment access management including UPS, MDB and DB, and on-site service management of the server room; 2.       Responsible for the daily operation and maintenance of power supply and all the cabling of the server room, and ensure a stable operation environment through implementation and management of strict and standard daily operation and maintenance on site. 3.       Responsible for capacity management of the server room, including floor plan and rack layout, ensuring optimal estate space usage of the server room; 4.       Responsible for coming up with the standard operating procedures for server room service management, and implementing the work quality management internally according to the standard to ensure improvement of work quality, timeliness and satisfaction; 5.       Responsible for daily operation and maintenance information management of the server room, identifying and providing server room space, power analysis report regularly, resolving all issues both existing and future. 6.       Serve as the duty leader, responsible for all work and personnel during the shift, and ensure the quality of work.","$2,500to$4,500",Monthly,Executive,data engineer
"TRIPLEONE SOMERSET, 111 SOMERSET ROAD 238164",PROPERTYGURU PTE. LTD.,"Permanent, Full Time","Engineering, Information Technology",Data Engineer,"Requirements Bachelor’s degree in IT or relevant field. Alternatively, lesser qualifications with strong experience in machine learning will also be considered  2+ years of industry experience in working with terabyte scale datasets Working knowledge of relational databases and query authoring (SQL) Experience with data workflow management tools such as Azkaban, Airflow Ability to write high performance quality code Experience in Python is a must. Other equivalent languages like C++, Java, Go, Scala is a plus. Experience with open source technologies like Kafka, Presto and Spark would be a plus  Awareness of various cloud-based solutions such as AWS Redshift, Google Big Query, Qubole is a plus  Visit us at: htps://www.propertygurugroup.com/","Roles & ResponsibilitiesOur websites attract more than 100 million monthly page-views which result in non-stop massive click-stream for behaviour data with around 130 million of users. As a result, PropertyGuru has the most comprehensive data for property supply and demand in Southeast Asia. Our Data Science team is empowered to build unique and compelling user experiences using machine learning and data.  Your work will be to design, develop, support and maintain our existing infrastructure and democratizing data access within and outside the company. Your work will support the following areas:  Real time streaming infrastructure: 	 Enable teams to move quickly and get accurate information to the right people with minimum delay would be the key focus of the data engineering team Advance our streaming platform which allows easy development of the streaming applications   Interactive Data Analytics: 	 Query the data and compute the aggregates on various dimensions to support the various decisions made on the data and machine learning products that are built around it   Infrastructure management: 	 Help manage multiple terabyte-scale clusters, easy-to-use systems to handle security and replication are in development   Data workflow management: 	 Use Airflow and Azkaban to schedule data related workflows supporting various analytical and machine learning workloads   Machine learning infrastructure: 	 Develop the end-to-end platform that will allow us to develop and deploy various machine learning models into the PropertyGuru websites and apps with ease Data engineers play a very big role in this platform development and has the potential to significantly cut down the development time of the machine learning models   ","$5,000to$7,000",Monthly,"Professional, Executive",data engineer
"TRIPLEONE SOMERSET, 111 SOMERSET ROAD 238164",PROPERTYGURU PTE. LTD.,"Permanent, Full Time","Engineering, Information Technology",Data Engineer,"Requirements Bachelor’s degree in IT or relevant field. Alternatively, lesser qualifications with strong experience in machine learning will also be considered  2+ years of industry experience in working with terabyte scale datasets Working knowledge of relational databases and query authoring (SQL) Experience with data workflow management tools such as Azkaban, Airflow Ability to write high performance quality code Experience in Python is a must. Other equivalent languages like C++, Java, Go, Scala is a plus. Experience with open source technologies like Kafka, Presto and Spark would be a plus  Awareness of various cloud-based solutions such as AWS Redshift, Google Big Query, Qubole is a plus  Visit us at: htps://www.propertygurugroup.com/","Roles & ResponsibilitiesOur websites attract more than 100 million monthly page-views which result in non-stop massive click-stream for behaviour data with around 130 million of users. As a result, PropertyGuru has the most comprehensive data for property supply and demand in Southeast Asia. Our Data Science team is empowered to build unique and compelling user experiences using machine learning and data.  Your work will be to design, develop, support and maintain our existing infrastructure and democratizing data access within and outside the company. Your work will support the following areas:  Real time streaming infrastructure:     Enable teams to move quickly and get accurate information to the right people with minimum delay would be the key focus of the data engineering team Advance our streaming platform which allows easy development of the streaming applications   Interactive Data Analytics:     Query the data and compute the aggregates on various dimensions to support the various decisions made on the data and machine learning products that are built around it   Infrastructure management:     Help manage multiple terabyte-scale clusters, easy-to-use systems to handle security and replication are in development   Data workflow management:     Use Airflow and Azkaban to schedule data related workflows supporting various analytical and machine learning workloads   Machine learning infrastructure:     Develop the end-to-end platform that will allow us to develop and deploy various machine learning models into the PropertyGuru websites and apps with ease Data engineers play a very big role in this platform development and has the potential to significantly cut down the development time of the machine learning models   ","$7,000to$9,000",Monthly,"Professional, Senior Executive",data engineer
,NEURONCREDIT PTE. LTD.,Full Time,Information Technology,Data Engineer,"RequirementsJob Requirements  B.Sc./M.Sc. in Computer Science or a related field At least 3-year experience as a Data Engineer or Backend Developer Hands-on development experience in python, Java and Scala Familiar with MySQL/Mariadb, MongoDB, and Redis. Hands-on experience of building applications and infrastructure. Hadoop/Spark eco-system or any other big data technologies is a plus Experience in Docker and K8s is a plus. Meticulous, responsible, a strong team player ","Roles & ResponsibilitiesJob Responsibilities  The Senior Data Engineer role requires maintaining data platforms, data pipeline and ETL, implementing AI models, and integrate our internal services. You will work with data scientists and back-end engineers in the team.","$5,000to$9,000",Monthly,Executive,data engineer
92 AMOY STREET 069911,ZUZU HOSPITALITY SOLUTIONS PTE. LTD.,Full Time,Engineering,Senior Data Engineer,"RequirementsResponsibilities: We are looking for a Sr. Data Engineer  to work on cloud and SaaS technologies. You will be responsible for the following:   Work on the collecting, storing, processing, and analyzing of huge sets of data.   Choose optimal solutions leveraging existing big data frameworks, then implement the infrastructure and  ETL processes, maintain and monitor them   Integrate these solutions with the existing architecture   Integrate these solutions with the existing CI/CD pipeline and automation   Propose architecture changes to handle the creation and consumption of big data.   Define data retention policies   Maintain high quality code   Investigate and, if necessary, prototype technologies relating to the task     Qualifications:   5-7 years of software development and data engineering experience   Proficient understanding of Big Data principles   Experience with integrating data from multiple sources and building Data Warehouses/ Data Lakes   Experience building stream processing systems leveraging Apache Storm, AWS Kinesis or Spark Streaming   Experience with Spark   Experience building and managing Hadoop clusters and proficient MapReduce and HDFS   Experience with NoSQL databases like MongoDB, DynamoDB , HBase or Cassandra   Knowledge of ETL techniques and frameworks like Flume or AWS Glue   Experience with  task schedulers like Airflow   Experience with ML toolkits like SparkML or H2O   Good understanding of Serverless Architecture   Ability to design and develop reusable and robust components and services   Experience with CI/CD using Jenkins is a plus   Team player with Agile development experience   BS or MS in Computer Science or related technical field  ","Roles & ResponsibilitiesJoin a visionary team with tremendous experience in the travel industry focused on delivering a product that will revolutionize the way hotels organize their business processes. If you are committed to the highest levels of achievement in cutting-edge engineering, we welcome you to help write the next chapter of our history. We're currently looking to hire an experienced Data Engineer  to join our team. The ideal candidate will handle a wide range of exciting tasks from prototyping new techniques and technologies, to implementing real world, customer-focused cloud-based solutions in markets that are ripe for picking As an early-stage start-up, the role will be very dynamic.  As such we need someone smart, quick-minded, ambitious, and flexible.  The role will grow as you grow!  ","$6,000to$10,000",Monthly,Professional,data engineer
"YELLOW PAGES BUILDING, 1 LORONG 2 TOA PAYOH 319637",HELIX LEISURE PTE. LTD.,Permanent,Information Technology,"Software Engineer, Data Services","RequirementsSkills & Qualifications:  BS degree in Computer Science preferred, similar technical field of study or equivalent practical experience will be considered Strong Core Java Development Experience Experience working with three or more from the following: web application development, Unix/Linux environments, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, and/or security software development Working proficiency and communication skills in verbal and written English Strong TSQL knowledge, able to optimise queries and understand unoptimised query plans Real Time, Multithreaded experience Experience with ORM tools such as hibernate Experience building high-volume file processing systems Experience with payments/transactions Experience in an Agile development environment. DBA experience Effective teamwork and good communication skills with the ability to mentor peers and provide peer code-reviews Ability to work effectively with software engineers to enhance test plans and automated testing framework ","Roles & ResponsibilitiesHelix Leisure is a leading global supplier to the Out of Home Entertainment industry – locations outside the home people visit for entertainment and recreation. Across our core brands – Embed (revenue management systems, e-commerce), Booking Boss (Tours, Attractions and Activities), LAI Games (arcade games), The Locker Network (operating electronic lockers) and Matahari Leisure (equipment manufacturing) we service over 2,500 locations around the globe. Helix operates full service offices in Singapore, Perth, Sydney, Dallas, Dubai and Jakarta. The group enables our customers to create rich experiences for their visitors and guests through both technology and service. As we embark on building the next generation platform for our software – a core consumer, supplier and distributor facing application, we are looking for highly motivated professionals who enjoy working in a fast paced, agile development environment. You will be working closely with product owners and UX designers to create and develop best-in-class data service solutions with the ability to use the latest in web development technology. Responsibilities:  Design, develop, test, deploy, maintain and improve software Manage individual project priorities, deadlines and deliverables ","$5,000to$9,000",Monthly,Senior Executive,data engineer
